#!/usr/bin/env bash
set -e

# ä¸€é”®å®‰è£… GPU æ¨¡å‹æœåŠ¡è„šæœ¬
# åŠŸèƒ½ï¼š
# - åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒ
# - å®‰è£… PyTorchï¼ˆéœ€æ‰‹åŠ¨æŒ‰ç¯å¢ƒé€‰æ‹©å‘½ä»¤ï¼‰
# - å®‰è£… GPU_server ä¾èµ–
# - é¢„ä¸‹è½½ embedding / rerank æ¨¡å‹
# - ç”Ÿæˆ Nginx åå‘ä»£ç†é…ç½®ï¼Œç»Ÿä¸€å¯¹å¤–ç«¯å£ 16000
#
# ä½¿ç”¨æ–¹æ³•ï¼ˆåœ¨ GPU æœåŠ¡å™¨ä¸Šï¼‰ï¼š
#   cd /opt/GPU_server
#   chmod +x install_gpu_server.sh
#   sudo ./install_gpu_server.sh
#
# å¯é€‰ç¯å¢ƒå˜é‡ï¼š
#   GPU_SERVER_PORT_INTERNAL   å†…éƒ¨ uvicorn ç«¯å£ï¼ˆé»˜è®¤ 18001ï¼‰
#   GPU_SERVER_PORT_PUBLIC     å¯¹å¤–æš´éœ²ç«¯å£ï¼ˆé»˜è®¤ 16000ï¼‰
#   GPU_SERVER_SERVER_NAME     Nginx server_nameï¼ˆé»˜è®¤ _ ï¼‰

GPU_SERVER_PORT_INTERNAL=${GPU_SERVER_PORT_INTERNAL:-18001}
GPU_SERVER_PORT_PUBLIC=${GPU_SERVER_PORT_PUBLIC:-16000}
GPU_SERVER_SERVER_NAME=${GPU_SERVER_SERVER_NAME:-_}

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
VENV_DIR="$ROOT_DIR/venv"
NGINX_CONF_PATH="/etc/nginx/conf.d/gpu_server.conf"

if [[ $EUID -ne 0 ]]; then
  echo "[ERROR] è¯·ä½¿ç”¨ root æƒé™è¿è¡Œæœ¬è„šæœ¬ï¼šsudo ./install_gpu_server.sh" >&2
  exit 1
fi

echo "[INFO] å·¥ä½œç›®å½•: $ROOT_DIR"

echo "[STEP] å®‰è£…åŸºç¡€ä¾èµ– (python3-venv, nginx, curl)"
if command -v apt-get >/dev/null 2>&1; then
  apt-get update -y
  apt-get install -y python3-venv nginx curl
else
  echo "[WARN] æœªæ£€æµ‹åˆ° apt-getï¼Œè¯·æ‰‹åŠ¨ç¡®ä¿å·²å®‰è£… python3-venvã€nginxã€curlã€‚"
fi

echo "[STEP] åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒ: $VENV_DIR"
if [[ ! -d "$VENV_DIR" ]]; then
  python3 -m venv "$VENV_DIR"
fi
# shellcheck disable=SC1090
source "$VENV_DIR/bin/activate"

echo "[STEP] å‡çº§ pip"
pip install --upgrade pip

echo "[STEP] æç¤ºï¼šè¯·æ ¹æ® GPU æœåŠ¡å™¨çš„ CUDA ç¯å¢ƒè‡ªè¡Œå®‰è£…åˆé€‚çš„ PyTorch ç‰ˆæœ¬"
echo "       ç¤ºä¾‹ (CUDA 11.8):"
echo "         pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 \\\n   --index-url https://download.pytorch.org/whl/cu118"
echo "       ç¤ºä¾‹ (ä»… CPU):"
echo "         pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0"
echo "       å¦‚éœ€å¯ç”¨ GPU OCRï¼ˆPaddleOCRï¼‰ï¼Œè¯·æ ¹æ®ç¯å¢ƒå®‰è£…å¯¹åº”çš„ paddlepaddle-gpu ç‰ˆæœ¬ï¼Œä¾‹å¦‚:"
echo "         pip install paddlepaddle-gpu -i https://mirror.baidu.com/pypi/simple"
echo
read -rp "[INPUT] å¦‚å·²å®‰è£… PyTorch æˆ–æš‚ä¸å®‰è£…ï¼Œç›´æ¥å›è½¦ç»§ç»­ï¼›å¦‚éœ€ç°åœ¨å®‰è£…ï¼Œè¯·æ‰‹åŠ¨æ‰§è¡Œä¸Šé¢çš„ pip å‘½ä»¤åå†å›è½¦..." _dummy

echo "[STEP] å®‰è£… GPU_server Python ä¾èµ–"
cd "$ROOT_DIR"
pip install -r requirements.txt

echo "[STEP] é…ç½® Hugging Face é•œåƒæº"
# è®¾ç½® Hugging Face é•œåƒæºç¯å¢ƒå˜é‡ï¼ˆå¦‚æœæœªè®¾ç½®ï¼‰
if [[ -z "${HF_ENDPOINT}" ]]; then
  # é»˜è®¤ä½¿ç”¨ hf-mirror.com é•œåƒæº
  export HF_ENDPOINT="${HF_MIRROR_ENDPOINT:-https://hf-mirror.com}"
  echo "[INFO] å·²è®¾ç½® HF_ENDPOINT=${HF_ENDPOINT}"
else
  echo "[INFO] ä½¿ç”¨å·²è®¾ç½®çš„ HF_ENDPOINT=${HF_ENDPOINT}"
fi

echo "[STEP] é¢„ä¸‹è½½ embedding / rerank / marker æ¨¡å‹ (é¦–æ¬¡ä¼šè¾ƒæ…¢)"
python - << 'PYCODE'
import os
from sentence_transformers import SentenceTransformer
from FlagEmbedding import FlagReranker

# ç¡®ä¿ä½¿ç”¨é•œåƒæº
if not os.getenv("HF_ENDPOINT"):
    os.environ["HF_ENDPOINT"] = os.getenv("HF_MIRROR_ENDPOINT", "https://hf-mirror.com")

print(f"[PY] ä½¿ç”¨ Hugging Face é•œåƒæº: {os.getenv('HF_ENDPOINT')}")

EMBED_MODEL = os.getenv("EMBED_MODEL_NAME", "BAAI/bge-large-zh-v1.5")
RERANK_MODEL = os.getenv("RERANKER_MODEL_NAME", "BAAI/bge-reranker-v2-m3")

print(f"[PY] é¢„ä¸‹è½½ embedding æ¨¡å‹: {EMBED_MODEL}")
SentenceTransformer(EMBED_MODEL)

print(f"[PY] é¢„ä¸‹è½½ reranker æ¨¡å‹: {RERANK_MODEL}")
FlagReranker(RERANK_MODEL, use_fp16=False)

print("[PY] æ¨¡å‹é¢„ä¸‹è½½å®Œæˆ")
PYCODE

echo "[STEP] ç”Ÿæˆ Nginx åå‘ä»£ç†é…ç½®: $NGINX_CONF_PATH"
cat > "$NGINX_CONF_PATH" <<EOF
# GPU æ¨¡å‹æœåŠ¡ - ç»Ÿä¸€å¯¹å¤–ç«¯å£ ${GPU_SERVER_PORT_PUBLIC}
upstream gpu_server_backend {
    server 127.0.0.1:${GPU_SERVER_PORT_INTERNAL};
    keepalive 32;
}

server {
    listen ${GPU_SERVER_PORT_PUBLIC} default_server;
    listen [::]:${GPU_SERVER_PORT_PUBLIC} default_server;
    server_name ${GPU_SERVER_SERVER_NAME};

    access_log /var/log/nginx/gpu_server_access.log;
    error_log /var/log/nginx/gpu_server_error.log;

    client_max_body_size 500M;

    location / {
        proxy_pass http://gpu_server_backend;
        proxy_http_version 1.1;

        proxy_set_header Host \$host;
        proxy_set_header X-Real-IP \$remote_addr;
        proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto \$scheme;

        proxy_read_timeout 1200s;
        proxy_connect_timeout 75s;
        proxy_send_timeout 300s;

        proxy_request_buffering off;
        proxy_buffering off;
    }
}
EOF

echo "[STEP] æ£€æŸ¥å¹¶é‡è½½ Nginx é…ç½®"
if command -v nginx >/dev/null 2>&1; then
  nginx -t
  if command -v systemctl >/dev/null 2>&1; then
    systemctl reload nginx || nginx -s reload
  else
    nginx -s reload || echo "[WARN] æ— æ³•è‡ªåŠ¨ reload nginxï¼Œè¯·æ‰‹åŠ¨æ‰§è¡Œ: nginx -s reload"
  fi
else
  echo "[WARN] æœªæ£€æµ‹åˆ° nginx å‘½ä»¤ï¼Œè¯·ç¡®è®¤ Nginx å·²æ­£ç¡®å®‰è£…å¹¶åœ¨ PATH ä¸­ã€‚"
fi

echo "[STEP] é…ç½®åå°å¯åŠ¨è„šæœ¬"
chmod +x "$ROOT_DIR/start_gpu_server.sh"
echo "[INFO] å¯åŠ¨è„šæœ¬å·²å‡†å¤‡: $ROOT_DIR/start_gpu_server.sh"

echo
echo "[å®Œæˆ] GPU æ¨¡å‹æœåŠ¡å®‰è£…æ­¥éª¤å·²å®Œæˆï¼"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  ä¸‹ä¸€æ­¥æ“ä½œ"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "1ï¸âƒ£  å¯åŠ¨æœåŠ¡ï¼ˆä¸‰ç§æ–¹å¼ä»»é€‰å…¶ä¸€ï¼‰ï¼š"
echo ""
echo "   æ–¹å¼ 1 - Systemd æœåŠ¡ç®¡ç†ï¼ˆæ¨èï¼Œè‡ªåŠ¨é‡å¯ã€å¼€æœºè‡ªå¯ï¼‰ï¼š"
echo "     sudo $ROOT_DIR/start_gpu_server.sh systemd"
echo ""
echo "   æ–¹å¼ 2 - åå°å¯åŠ¨ï¼ˆé€‚åˆæ—  systemd ç¯å¢ƒï¼‰ï¼š"
echo "     $ROOT_DIR/start_gpu_server.sh background"
echo ""
echo "   æ–¹å¼ 3 - å‰å°å¯åŠ¨ï¼ˆç”¨äºè°ƒè¯•ï¼‰ï¼š"
echo "     cd $ROOT_DIR"
echo "     source venv/bin/activate"
echo "     uvicorn main:app --host 0.0.0.0 --port ${GPU_SERVER_PORT_INTERNAL} --workers 1"
echo ""
echo "2ï¸âƒ£  é…ç½®å¼€æœºè‡ªå¯ï¼ˆå¦‚æœæ²¡æœ‰ systemdï¼‰ï¼š"
echo ""
echo "     sudo $ROOT_DIR/start_gpu_server.sh enable-autostart"
echo ""
echo "3ï¸âƒ£  æœåŠ¡ç®¡ç†å‘½ä»¤ï¼š"
echo ""
echo "     æŸ¥çœ‹çŠ¶æ€: $ROOT_DIR/start_gpu_server.sh status"
echo "     åœæ­¢æœåŠ¡: $ROOT_DIR/start_gpu_server.sh stop"
echo "     é‡å¯æœåŠ¡: $ROOT_DIR/start_gpu_server.sh restart"
echo ""
if command -v systemctl >/dev/null 2>&1; then
echo "   ï¼ˆå¦‚æœä½¿ç”¨ systemdï¼‰ï¼š"
echo "     æŸ¥çœ‹çŠ¶æ€: systemctl status gpu-server"
echo "     æŸ¥çœ‹æ—¥å¿—: journalctl -u gpu-server -f"
echo "     åœæ­¢æœåŠ¡: systemctl stop gpu-server"
echo "     é‡å¯æœåŠ¡: systemctl restart gpu-server"
echo ""
fi
echo "4ï¸âƒ£  åœ¨ä¸»ä¸šåŠ¡æœåŠ¡å™¨ä¸Šé…ç½®ç¯å¢ƒå˜é‡ï¼š"
echo ""
echo "     export GPU_MODEL_SERVER_URL=\"http://<GPU_SERVER_IP>:${GPU_SERVER_PORT_PUBLIC}\""
echo ""
echo "5ï¸âƒ£  æµ‹è¯•æœåŠ¡ï¼š"
echo ""
echo "     curl http://localhost:${GPU_SERVER_PORT_PUBLIC}/health"
echo "     curl http://localhost:${GPU_SERVER_PORT_PUBLIC}/docs"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“– æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ README.md"
echo ""
