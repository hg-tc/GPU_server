"""
GPU Model Server - PaddleOCR 3.x Version
=========================================
æ”¯æŒ:
- PDF è½¬ Markdown (PP-StructureV3)
- å›¾ç‰‡ OCR (PaddleOCR 3.x)
- æ–‡æ¡£ç‰ˆé¢åˆ†æ (PP-StructureV3)
- æ–‡æœ¬åµŒå…¥ (sentence-transformers)
- æ–‡æœ¬é‡æ’åº (FlagEmbedding)
"""

import logging
import os
import tempfile
import time
from datetime import datetime
from typing import List, Optional, Dict, Any
from pathlib import Path
import base64
import io

from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.exceptions import HTTPException as StarletteHTTPException
from pydantic import BaseModel
import torch

# ============================================
# æ—¥å¿—é…ç½®
# ============================================
def setup_logging():
    """é…ç½®è¯¦ç»†çš„æ—¥å¿—ç³»ç»Ÿ"""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    log_format = "%(asctime)s [%(levelname)-8s] [%(name)s] %(message)s"
    date_format = "%Y-%m-%d %H:%M:%S"
    
    # ä¸»æ—¥å¿—æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¯ç”¨å®æ—¶åˆ·æ–°ï¼‰
    file_handler = logging.FileHandler(
        log_dir / "gpu_server.log",
        encoding="utf-8",
        mode="a"
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(logging.Formatter(log_format, date_format))
    # è®¾ç½®å»¶è¿Ÿä¸º Falseï¼Œæ¯æ¬¡å†™å…¥åç«‹å³åˆ·æ–°
    if hasattr(file_handler, 'stream'):
        file_handler.stream.reconfigure(line_buffering=True)
    
    # é”™è¯¯æ—¥å¿—æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¯ç”¨å®æ—¶åˆ·æ–°ï¼‰
    error_handler = logging.FileHandler(
        log_dir / "gpu_server_error.log",
        encoding="utf-8",
        mode="a"
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(logging.Formatter(log_format, date_format))
    # è®¾ç½®å»¶è¿Ÿä¸º Falseï¼Œæ¯æ¬¡å†™å…¥åç«‹å³åˆ·æ–°
    if hasattr(error_handler, 'stream'):
        error_handler.stream.reconfigure(line_buffering=True)
    
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.handlers.clear()
    root_logger.addHandler(file_handler)
    root_logger.addHandler(error_handler)
    root_logger.addHandler(console_handler)
    
    # ä¸ºæ–‡ä»¶å¤„ç†å™¨æ·»åŠ è‡ªåŠ¨åˆ·æ–°æœºåˆ¶
    for handler in [file_handler, error_handler]:
        original_emit = handler.emit
        def make_flush_emit(h):
            def flush_emit(record):
                result = original_emit(record)
                if hasattr(h, 'stream') and hasattr(h.stream, 'flush'):
                    h.stream.flush()
                return result
            return flush_emit
        handler.emit = make_flush_emit(handler)
    
    logger = logging.getLogger("gpu_server")
    logger.setLevel(logging.DEBUG)
    
    # é™ä½ç¬¬ä¸‰æ–¹åº“æ—¥å¿—çº§åˆ«
    logging.getLogger("uvicorn").setLevel(logging.INFO)
    logging.getLogger("uvicorn.access").setLevel(logging.INFO)
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("httpcore").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("paddle").setLevel(logging.WARNING)
    
    return logger

logger = setup_logging()

# ============================================
# GPU æ£€æµ‹
# ============================================
def detect_device():
    """æ£€æµ‹å¯ç”¨è®¾å¤‡"""
    # æ£€æŸ¥ PaddlePaddle GPU
    try:
        import paddle
        if paddle.device.is_compiled_with_cuda():
            gpu_count = paddle.device.cuda.device_count()
            if gpu_count > 0:
                logger.info("=" * 60)
                logger.info("ğŸš€ PaddlePaddle GPU æ£€æµ‹:")
                logger.info(f"  âœ… CUDA å¯ç”¨")
                logger.info(f"  ğŸ® GPU æ•°é‡: {gpu_count}")
                for i in range(gpu_count):
                    props = paddle.device.cuda.get_device_properties(i)
                    logger.info(f"  ğŸ¯ GPU {i}: {props.name}")
                logger.info("=" * 60)
                return "gpu"
    except Exception as e:
        logger.warning(f"PaddlePaddle GPU æ£€æµ‹å¤±è´¥: {e}")
    
    # æ£€æŸ¥ PyTorch GPU
    try:
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            logger.info("=" * 60)
            logger.info("ğŸš€ PyTorch GPU æ£€æµ‹:")
            logger.info(f"  âœ… CUDA å¯ç”¨")
            logger.info(f"  ğŸ® GPU æ•°é‡: {gpu_count}")
            for i in range(gpu_count):
                props = torch.cuda.get_device_properties(i)
                logger.info(f"  ğŸ¯ GPU {i}: {props.name}")
                logger.info(f"     æ˜¾å­˜: {props.total_memory / 1024**3:.2f} GB")
            logger.info("=" * 60)
            return "cuda"
    except Exception as e:
        logger.warning(f"PyTorch GPU æ£€æµ‹å¤±è´¥: {e}")
    
    logger.warning("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPU")
    return "cpu"

DEVICE = detect_device()

# ============================================
# FastAPI åº”ç”¨
# ============================================
app = FastAPI(
    title="GPU Model Server - PaddleOCR 3.x",
    version="3.0.0",
    description="PDF/å›¾ç‰‡å¤„ç†ã€OCRã€åµŒå…¥ã€é‡æ’åºæœåŠ¡",
)

# è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
class RequestLoggingMiddleware(BaseHTTPMiddleware):
    WARNING_TIMEOUT = 60
    CRITICAL_TIMEOUT = 300
    
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        client_ip = request.client.host if request.client else "unknown"
        
        logger.info(f"[è¯·æ±‚å¼€å§‹] {request.method} {request.url.path} | å®¢æˆ·ç«¯: {client_ip}")
        
        try:
            response = await call_next(request)
            process_time = time.time() - start_time
            
            if process_time > self.CRITICAL_TIMEOUT:
                logger.warning(f"[è¯·æ±‚è¶…æ—¶] âš ï¸âš ï¸âš ï¸ {request.url.path} | è€—æ—¶: {process_time:.3f}s")
            elif process_time > self.WARNING_TIMEOUT:
                logger.warning(f"[è¯·æ±‚è¶…æ—¶] âš ï¸ {request.url.path} | è€—æ—¶: {process_time:.3f}s")
            
            status_emoji = "âœ…" if 200 <= response.status_code < 300 else "âŒ"
            logger.info(f"[è¯·æ±‚å®Œæˆ] {status_emoji} {request.url.path} | çŠ¶æ€: {response.status_code} | è€—æ—¶: {process_time:.3f}s")
            
            return response
        except Exception as e:
            process_time = time.time() - start_time
            logger.error(f"[è¯·æ±‚å¼‚å¸¸] âŒ {request.url.path} | é”™è¯¯: {e} | è€—æ—¶: {process_time:.3f}s", exc_info=True)
            raise

app.add_middleware(RequestLoggingMiddleware)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å¼‚å¸¸å¤„ç†
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"[å…¨å±€å¼‚å¸¸] {request.url.path} | {type(exc).__name__}: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": f"Internal server error: {str(exc)}", "error_type": type(exc).__name__}
    )

@app.exception_handler(StarletteHTTPException)
async def http_exception_handler(request: Request, exc: StarletteHTTPException):
    return JSONResponse(status_code=exc.status_code, content={"detail": exc.detail})

# ============================================
# æ¨¡å‹å•ä¾‹ç®¡ç†
# ============================================
_ocr_engine = None
_structure_engine = None
_embedder = None
_reranker = None

def _get_ocr_engine():
    """è·å– PaddleOCR 3.x å¼•æ“"""
    global _ocr_engine
    if _ocr_engine is None:
        try:
            from paddleocr import PaddleOCR
            logger.info("åˆå§‹åŒ– PaddleOCR 3.x...")
            
            device = "gpu" if DEVICE in ["gpu", "cuda"] else "cpu"
            _ocr_engine = PaddleOCR(
                use_doc_orientation_classify=False,
                use_doc_unwarping=False,
                use_textline_orientation=False,
                device=device,
            )
            logger.info(f"PaddleOCR 3.x åˆå§‹åŒ–æˆåŠŸ | device={device}")
        except Exception as e:
            logger.error(f"PaddleOCR åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    return _ocr_engine


def _get_structure_engine():
    """è·å– PP-StructureV3 å¼•æ“"""
    global _structure_engine
    if _structure_engine is None:
        try:
            from paddleocr import PPStructureV3
            logger.info("åˆå§‹åŒ– PP-StructureV3...")
            
            device = "gpu" if DEVICE in ["gpu", "cuda"] else "cpu"
            _structure_engine = PPStructureV3(
                use_doc_orientation_classify=False,
                use_doc_unwarping=False,
                device=device,
            )
            logger.info(f"PP-StructureV3 åˆå§‹åŒ–æˆåŠŸ | device={device}")
        except Exception as e:
            logger.error(f"PP-StructureV3 åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    return _structure_engine


def _get_embedder():
    """è·å–åµŒå…¥æ¨¡å‹"""
    global _embedder
    if _embedder is None:
        try:
            from sentence_transformers import SentenceTransformer
            
            # è®¾ç½® HuggingFace ç¦»çº¿æ¨¡å¼ï¼ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰
            os.environ["HF_HUB_OFFLINE"] = "1"
            os.environ["TRANSFORMERS_OFFLINE"] = "1"
            
            model_name = os.getenv("EMBED_MODEL_NAME", "BAAI/bge-large-zh-v1.5")
            device = "cuda" if DEVICE in ["gpu", "cuda"] else "cpu"
            
            logger.info(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {model_name} | device={device} | ç¦»çº¿æ¨¡å¼ï¼ˆä»…æœ¬åœ°æ–‡ä»¶ï¼‰")
            # ä½¿ç”¨ local_files_only=True å¼ºåˆ¶åªä½¿ç”¨æœ¬åœ°ç¼“å­˜
            _embedder = SentenceTransformer(model_name, device=device, local_files_only=True)
            logger.info("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    return _embedder


def _get_reranker():
    """è·å–é‡æ’åºæ¨¡å‹"""
    global _reranker
    if _reranker is None:
        try:
            from FlagEmbedding import FlagReranker
            import glob
            
            # è®¾ç½® HuggingFace ç¦»çº¿æ¨¡å¼ï¼ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰
            os.environ["HF_HUB_OFFLINE"] = "1"
            os.environ["TRANSFORMERS_OFFLINE"] = "1"
            
            model_name = os.getenv("RERANKER_MODEL_NAME", "BAAI/bge-reranker-v2-m3")
            device = "cuda" if DEVICE in ["gpu", "cuda"] else "cpu"
            use_fp16 = device == "cuda"
            
            # å°è¯•æ‰¾åˆ°æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹è·¯å¾„
            hf_home = os.getenv("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
            cache_dir = os.path.join(hf_home, "hub")
            model_cache_name = f"models--{model_name.replace('/', '--')}"
            model_cache_path = os.path.join(cache_dir, model_cache_name, "snapshots")
            
            # æŸ¥æ‰¾æœ€æ–°çš„å¿«ç…§ç›®å½•
            local_model_path = None
            if os.path.exists(model_cache_path):
                snapshots = sorted(glob.glob(os.path.join(model_cache_path, "*")), reverse=True)
                if snapshots and os.path.isdir(snapshots[0]):
                    local_model_path = snapshots[0]
                    logger.info(f"æ‰¾åˆ°æœ¬åœ°æ¨¡å‹ç¼“å­˜: {local_model_path}")
            
            # å¦‚æœæ‰¾åˆ°æœ¬åœ°è·¯å¾„ï¼Œä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼›å¦åˆ™ä½¿ç”¨æ¨¡å‹åç§°ï¼ˆä¾èµ–ç¯å¢ƒå˜é‡ï¼‰
            if local_model_path:
                logger.info(f"åˆå§‹åŒ–é‡æ’åºæ¨¡å‹: {local_model_path} | device={device} | ç¦»çº¿æ¨¡å¼ï¼ˆæœ¬åœ°è·¯å¾„ï¼‰")
                _reranker = FlagReranker(
                    local_model_path,
                    use_fp16=use_fp16,
                    devices=device
                )
            else:
                logger.info(f"åˆå§‹åŒ–é‡æ’åºæ¨¡å‹: {model_name} | device={device} | ç¦»çº¿æ¨¡å¼ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰")
                _reranker = FlagReranker(
                    model_name,
                    use_fp16=use_fp16,
                    devices=device,
                    cache_dir=cache_dir
                )
            logger.info("é‡æ’åºæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            from sentence_transformers import CrossEncoder
            logger.warning("FlagEmbedding ä¸å¯ç”¨ï¼Œä½¿ç”¨ CrossEncoder")
            _reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-12-v2", device=device)
        except Exception as e:
            logger.error(f"é‡æ’åºæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    return _reranker

# ============================================
# ç»Ÿè®¡ä¿¡æ¯
# ============================================
_server_start_time = time.time()
_request_stats = {
    "ocr_total": 0, "ocr_success": 0, "ocr_failed": 0,
    "structure_total": 0, "structure_success": 0, "structure_failed": 0,
    "pdf_total": 0, "pdf_success": 0, "pdf_failed": 0,
    "embed_total": 0, "embed_success": 0, "embed_failed": 0,
    "rerank_total": 0, "rerank_success": 0, "rerank_failed": 0,
}

# ============================================
# API ç«¯ç‚¹
# ============================================

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    status = {
        "status": "ok",
        "timestamp": datetime.now().isoformat(),
        "version": "3.0.0",
        "device": DEVICE,
    }
    
    # GPU ä¿¡æ¯
    try:
        if torch.cuda.is_available():
            status["gpu"] = {
                "available": True,
                "device_count": torch.cuda.device_count(),
                "device_name": torch.cuda.get_device_name(0),
                "memory": {
                    "allocated_gb": round(torch.cuda.memory_allocated(0) / 1024**3, 2),
                    "reserved_gb": round(torch.cuda.memory_reserved(0) / 1024**3, 2),
                    "total_gb": round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2),
                }
            }
        else:
            status["gpu"] = {"available": False}
    except Exception as e:
        status["gpu"] = {"available": False, "error": str(e)}
    
    # æ¨¡å‹çŠ¶æ€ï¼ˆæ‡’åŠ è½½ï¼šåªæœ‰åœ¨ä½¿ç”¨æ—¶æ‰ä¼šåŠ è½½ï¼‰
    status["models"] = {
        "ocr": "loaded" if _ocr_engine is not None else "lazy",
        "structure": "loaded" if _structure_engine is not None else "lazy",
        "embedder": "loaded" if _embedder is not None else "lazy",
        "reranker": "loaded" if _reranker is not None else "lazy",
    }
    
    return status


@app.get("/stats")
async def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "stats": _request_stats,
        "uptime_seconds": time.time() - _server_start_time,
    }


@app.post("/clear_cache")
async def clear_gpu_cache():
    """æ¸…ç† GPU ç¼“å­˜"""
    try:
        if torch.cuda.is_available():
            before = torch.cuda.memory_allocated(0) / 1024**3
            torch.cuda.empty_cache()
            after = torch.cuda.memory_allocated(0) / 1024**3
            return {"status": "ok", "freed_gb": round(before - after, 2)}
        return {"status": "ok", "message": "No GPU"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============================================
# OCR ç«¯ç‚¹
# ============================================

class OCRResponse(BaseModel):
    text: str
    confidence: float
    lines: List[str]
    confidences: List[float]
    boxes: List[List[List[float]]]


@app.post("/ocr_image", response_model=OCRResponse)
async def ocr_image(file: UploadFile = File(...)) -> OCRResponse:
    """å›¾ç‰‡ OCR - ä½¿ç”¨ PaddleOCR 3.x"""
    global _request_stats
    _request_stats["ocr_total"] += 1
    
    start_time = time.time()
    tmp_path = None
    
    try:
        if not file.filename:
            raise HTTPException(status_code=400, detail="Empty filename")
        
        suffix = os.path.splitext(file.filename)[1].lower() or ".png"
        
        with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
            tmp_path = tmp.name
            content = await file.read()
            if not content:
                raise HTTPException(status_code=400, detail="Empty file")
            tmp.write(content)
            tmp.flush()  # ç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜
            os.fsync(tmp.fileno())  # å¼ºåˆ¶åŒæ­¥åˆ°ç£ç›˜
        
        ocr = _get_ocr_engine()
        
        # PaddleOCR 3.x ä½¿ç”¨ predict æ–¹æ³•
        results = ocr.predict(tmp_path)
        
        lines = []
        confidences = []
        boxes = []
        
        # è°ƒè¯•ï¼šè®°å½•åŸå§‹ç»“æœç±»å‹å’Œå†…å®¹
        logger.debug(f"[OCR] ç»“æœç±»å‹: {type(results)}, ç»“æœæ•°é‡: {len(results) if hasattr(results, '__len__') else 'N/A'}")
        if isinstance(results, list) and len(results) > 0:
            logger.debug(f"[OCR] ç¬¬ä¸€ä¸ªç»“æœç±»å‹: {type(results[0])}, å†…å®¹: {str(results[0])[:200]}")
            if hasattr(results[0], '__dict__'):
                logger.debug(f"[OCR] ç¬¬ä¸€ä¸ªç»“æœå±æ€§: {list(results[0].__dict__.keys())}")
        
        # å¤„ç†ç»“æœ - PaddleOCR 3.x å¯èƒ½è¿”å›ä¸åŒçš„æ ¼å¼
        if isinstance(results, list):
            for idx, res in enumerate(results):
                logger.debug(f"[OCR] å¤„ç†ç»“æœ {idx}: ç±»å‹={type(res)}")
                # æ–¹æ³•1: æ£€æŸ¥æ˜¯å¦æœ‰ rec_texts å±æ€§
                if hasattr(res, 'rec_texts'):
                    rec_texts = res.rec_texts if hasattr(res, 'rec_texts') else []
                    rec_scores = res.rec_scores if hasattr(res, 'rec_scores') else []
                    rec_polys = res.rec_polys if hasattr(res, 'rec_polys') else []
                    
                    for i, text in enumerate(rec_texts):
                        if text and text.strip():
                            lines.append(text.strip())
                            confidences.append(float(rec_scores[i]) if i < len(rec_scores) else 0.0)
                            if i < len(rec_polys):
                                boxes.append(rec_polys[i].tolist() if hasattr(rec_polys[i], 'tolist') else rec_polys[i])
                
                # æ–¹æ³•2: æ£€æŸ¥æ˜¯å¦æœ‰ json å±æ€§
                elif hasattr(res, 'json'):
                    data = res.json
                    if isinstance(data, dict) and 'rec_texts' in data:
                        for i, text in enumerate(data['rec_texts']):
                            if text and text.strip():
                                lines.append(text.strip())
                                confidences.append(float(data['rec_scores'][i]) if i < len(data.get('rec_scores', [])) else 0.0)
                
                # æ–¹æ³•3: ç›´æ¥æ˜¯å­—å…¸æ ¼å¼
                elif isinstance(res, dict):
                    if 'rec_texts' in res:
                        for i, text in enumerate(res['rec_texts']):
                            if text and text.strip():
                                lines.append(text.strip())
                                confidences.append(float(res['rec_scores'][i]) if i < len(res.get('rec_scores', [])) else 0.0)
                                if 'rec_polys' in res and i < len(res['rec_polys']):
                                    boxes.append(res['rec_polys'][i])
                
                # æ–¹æ³•4: å°è¯•ç›´æ¥è®¿é—®æ–‡æœ¬ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬æ ¼å¼ï¼‰
                elif isinstance(res, (list, tuple)) and len(res) >= 2:
                    # æ ¼å¼: [[[x1,y1], [x2,y2], ...], (text, confidence)]
                    for item in res:
                        if isinstance(item, (list, tuple)) and len(item) >= 2:
                            text = item[1][0] if isinstance(item[1], (list, tuple)) else str(item[1])
                            conf = item[1][1] if isinstance(item[1], (list, tuple)) and len(item[1]) > 1 else 0.0
                            if text and text.strip():
                                lines.append(text.strip())
                                confidences.append(float(conf))
                                if isinstance(item[0], (list, tuple)):
                                    boxes.append(item[0])
        
        logger.debug(f"[OCR] è§£æç»“æœ: {len(lines)} è¡Œæ–‡æœ¬")
        
        full_text = "\n".join(lines)
        avg_conf = sum(confidences) / len(confidences) if confidences else 0.0
        
        elapsed = time.time() - start_time
        logger.info(f"[OCR] âœ… å®Œæˆ | æ–‡ä»¶: {file.filename} | æ–‡æœ¬é•¿åº¦: {len(full_text)} | ç½®ä¿¡åº¦: {avg_conf:.4f} | è€—æ—¶: {elapsed:.3f}s")
        
        _request_stats["ocr_success"] += 1
        return OCRResponse(
            text=full_text,
            confidence=avg_conf,
            lines=lines,
            confidences=confidences,
            boxes=boxes,
        )
    
    except HTTPException:
        _request_stats["ocr_failed"] += 1
        raise
    except Exception as e:
        _request_stats["ocr_failed"] += 1
        logger.error(f"[OCR] âŒ å¤±è´¥ | æ–‡ä»¶: {file.filename} | é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"OCR failed: {e}")
    finally:
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except:
                pass


class OCRBase64Request(BaseModel):
    image_base64: str
    filename: Optional[str] = "image.png"


@app.post("/ocr_base64", response_model=OCRResponse)
async def ocr_base64(request: OCRBase64Request) -> OCRResponse:
    """Base64 å›¾ç‰‡ OCR"""
    global _request_stats
    _request_stats["ocr_total"] += 1
    
    start_time = time.time()
    tmp_path = None
    
    try:
        # è§£ç  base64
        image_data = base64.b64decode(request.image_base64)
        
        suffix = os.path.splitext(request.filename)[1].lower() or ".png"
        
        with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
            tmp_path = tmp.name
            tmp.write(image_data)
            tmp.flush()  # ç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜
            os.fsync(tmp.fileno())  # å¼ºåˆ¶åŒæ­¥åˆ°ç£ç›˜
        
        ocr = _get_ocr_engine()
        results = ocr.predict(tmp_path)
        
        lines = []
        confidences = []
        boxes = []
        
        # ä½¿ç”¨ä¸ ocr_image ç›¸åŒçš„è§£æé€»è¾‘
        if isinstance(results, list):
            for idx, res in enumerate(results):
                logger.debug(f"[OCR-Base64] å¤„ç†ç»“æœ {idx}: ç±»å‹={type(res)}")
                if hasattr(res, 'rec_texts'):
                    rec_texts = res.rec_texts if hasattr(res, 'rec_texts') else []
                    rec_scores = res.rec_scores if hasattr(res, 'rec_scores') else []
                    
                    for i, text in enumerate(rec_texts):
                        if text and text.strip():
                            lines.append(text.strip())
                            confidences.append(float(rec_scores[i]) if i < len(rec_scores) else 0.0)
                elif isinstance(res, dict) and 'rec_texts' in res:
                    for i, text in enumerate(res['rec_texts']):
                        if text and text.strip():
                            lines.append(text.strip())
                            confidences.append(float(res['rec_scores'][i]) if i < len(res.get('rec_scores', [])) else 0.0)
                elif isinstance(res, (list, tuple)) and len(res) >= 2:
                    for item in res:
                        if isinstance(item, (list, tuple)) and len(item) >= 2:
                            text = item[1][0] if isinstance(item[1], (list, tuple)) else str(item[1])
                            conf = item[1][1] if isinstance(item[1], (list, tuple)) and len(item[1]) > 1 else 0.0
                            if text and text.strip():
                                lines.append(text.strip())
                                confidences.append(float(conf))
        
        full_text = "\n".join(lines)
        avg_conf = sum(confidences) / len(confidences) if confidences else 0.0
        
        elapsed = time.time() - start_time
        logger.info(f"[OCR-Base64] âœ… å®Œæˆ | æ–‡æœ¬é•¿åº¦: {len(full_text)} | è€—æ—¶: {elapsed:.3f}s")
        
        _request_stats["ocr_success"] += 1
        return OCRResponse(
            text=full_text,
            confidence=avg_conf,
            lines=lines,
            confidences=confidences,
            boxes=boxes,
        )
    
    except Exception as e:
        _request_stats["ocr_failed"] += 1
        logger.error(f"[OCR-Base64] âŒ å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"OCR failed: {e}")
    finally:
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except:
                pass


# ============================================
# æ–‡æ¡£ç»“æ„åˆ†æç«¯ç‚¹
# ============================================

class StructureResponse(BaseModel):
    markdown: str
    layout_info: Optional[Dict[str, Any]] = None


@app.post("/structure_image", response_model=StructureResponse)
async def structure_image(file: UploadFile = File(...)) -> StructureResponse:
    """å›¾ç‰‡ç‰ˆé¢åˆ†æ - ä½¿ç”¨ PP-StructureV3"""
    global _request_stats
    _request_stats["structure_total"] += 1
    
    start_time = time.time()
    tmp_path = None
    
    try:
        suffix = os.path.splitext(file.filename)[1].lower() or ".png"
        
        with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
            tmp_path = tmp.name
            content = await file.read()
            tmp.write(content)
        
        structure = _get_structure_engine()
        results = structure.predict(tmp_path)
        
        markdown_text = ""
        layout_info = {}
        
        for res in results:
            if hasattr(res, 'markdown'):
                md_info = res.markdown
                if isinstance(md_info, dict):
                    markdown_text = md_info.get('markdown_text', '')
                elif isinstance(md_info, str):
                    markdown_text = md_info
            
            if hasattr(res, 'json'):
                layout_info = res.json
        
        elapsed = time.time() - start_time
        logger.info(f"[Structure] âœ… å®Œæˆ | æ–‡ä»¶: {file.filename} | Markdowné•¿åº¦: {len(markdown_text)} | è€—æ—¶: {elapsed:.3f}s")
        
        _request_stats["structure_success"] += 1
        return StructureResponse(markdown=markdown_text, layout_info=layout_info)
    
    except Exception as e:
        _request_stats["structure_failed"] += 1
        logger.error(f"[Structure] âŒ å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Structure analysis failed: {e}")
    finally:
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except:
                pass


@app.post("/structure_pdf")
async def structure_pdf(file: UploadFile = File(...)):
    """PDF ç‰ˆé¢åˆ†æ - ä½¿ç”¨ PP-StructureV3"""
    global _request_stats
    _request_stats["structure_total"] += 1
    
    start_time = time.time()
    tmp_path = None
    
    try:
        if not file.filename.lower().endswith(".pdf"):
            raise HTTPException(status_code=400, detail="Only PDF files supported")
        
        with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp:
            tmp_path = tmp.name
            content = await file.read()
            tmp.write(content)
        
        structure = _get_structure_engine()
        results = structure.predict(tmp_path)
        
        # åˆå¹¶æ‰€æœ‰é¡µé¢çš„ Markdown
        markdown_list = []
        for res in results:
            if hasattr(res, 'markdown'):
                md_info = res.markdown
                markdown_list.append(md_info)
        
        # ä½¿ç”¨ PP-StructureV3 çš„åˆå¹¶æ–¹æ³•
        if hasattr(structure, 'concatenate_markdown_pages'):
            full_markdown = structure.concatenate_markdown_pages(markdown_list)
        else:
            full_markdown = "\n\n---\n\n".join([
                m.get('markdown_text', '') if isinstance(m, dict) else str(m)
                for m in markdown_list
            ])
        
        elapsed = time.time() - start_time
        logger.info(f"[Structure-PDF] âœ… å®Œæˆ | æ–‡ä»¶: {file.filename} | é¡µæ•°: {len(markdown_list)} | è€—æ—¶: {elapsed:.3f}s")
        
        _request_stats["structure_success"] += 1
        return {
            "content": full_markdown,
            "page_count": len(markdown_list),
            "conversion_method": "PP-StructureV3",
        }
    
    except HTTPException:
        _request_stats["structure_failed"] += 1
        raise
    except Exception as e:
        _request_stats["structure_failed"] += 1
        logger.error(f"[Structure-PDF] âŒ å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"PDF structure analysis failed: {e}")
    finally:
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except:
                pass


# ============================================
# PDF è½¬ Markdown ç«¯ç‚¹
# ============================================

@app.post("/pdf_to_markdown")
async def pdf_to_markdown(file: UploadFile = File(...)):
    """PDF è½¬ Markdown (ä½¿ç”¨ PP-StructureV3)"""
    global _request_stats
    _request_stats["pdf_total"] += 1
    
    start_time = time.time()
    tmp_path = None
    
    try:
        if not file.filename.lower().endswith(".pdf"):
            raise HTTPException(status_code=400, detail="Only PDF files supported")
        
        with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp:
            tmp_path = tmp.name
            content = await file.read()
            file_size = len(content)
            tmp.write(content)
        
        logger.info(f"[PDF] å¼€å§‹å¤„ç† | æ–‡ä»¶: {file.filename} | å¤§å°: {file_size/1024/1024:.2f}MB")
        
        # ä½¿ç”¨ PP-StructureV3
        structure = _get_structure_engine()
        results = structure.predict(tmp_path)
        
        markdown_list = []
        for res in results:
            if hasattr(res, 'markdown'):
                markdown_list.append(res.markdown)
        
        if hasattr(structure, 'concatenate_markdown_pages'):
            markdown = structure.concatenate_markdown_pages(markdown_list)
        else:
            markdown = "\n\n---\n\n".join([
                m.get('markdown_text', '') if isinstance(m, dict) else str(m)
                for m in markdown_list
            ])
        
        if not markdown:
            raise HTTPException(status_code=500, detail="Empty output")
        
        elapsed = time.time() - start_time
        logger.info(f"[PDF] âœ… å®Œæˆ | æ–‡ä»¶: {file.filename} | è¾“å‡º: {len(markdown)} chars | è€—æ—¶: {elapsed:.3f}s")
        
        _request_stats["pdf_success"] += 1
        return {
            "content": markdown,
            "conversion_method": "PP-StructureV3",
            "file_name": file.filename,
        }
    
    except HTTPException:
        _request_stats["pdf_failed"] += 1
        raise
    except Exception as e:
        _request_stats["pdf_failed"] += 1
        logger.error(f"[PDF] âŒ å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"PDF conversion failed: {e}")
    finally:
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except:
                pass


# ============================================
# åµŒå…¥ç«¯ç‚¹
# ============================================

class EmbedRequest(BaseModel):
    texts: List[str]


class EmbedResponse(BaseModel):
    embeddings: List[List[float]]


@app.post("/embed", response_model=EmbedResponse)
async def embed(request: EmbedRequest) -> EmbedResponse:
    """æ–‡æœ¬åµŒå…¥"""
    global _request_stats
    _request_stats["embed_total"] += 1
    
    start_time = time.time()
    
    if not request.texts:
        return EmbedResponse(embeddings=[])
    
    try:
        model = _get_embedder()
        vectors = model.encode(request.texts, normalize_embeddings=True)
        embeddings = vectors.tolist()
        
        elapsed = time.time() - start_time
        logger.info(f"[Embed] âœ… å®Œæˆ | æ–‡æœ¬æ•°: {len(request.texts)} | è€—æ—¶: {elapsed:.3f}s")
        
        _request_stats["embed_success"] += 1
        return EmbedResponse(embeddings=embeddings)
    
    except Exception as e:
        _request_stats["embed_failed"] += 1
        logger.error(f"[Embed] âŒ å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Embedding failed: {e}")


class BatchEmbedRequest(BaseModel):
    batches: List[List[str]]


class BatchEmbedResponse(BaseModel):
    embeddings: List[List[List[float]]]
    batch_times: List[float]


@app.post("/embed_batch", response_model=BatchEmbedResponse)
async def embed_batch(request: BatchEmbedRequest) -> BatchEmbedResponse:
    """æ‰¹é‡æ–‡æœ¬åµŒå…¥"""
    global _request_stats
    _request_stats["embed_total"] += 1
    
    if not request.batches:
        return BatchEmbedResponse(embeddings=[], batch_times=[])
    
    try:
        model = _get_embedder()
        all_embeddings = []
        batch_times = []
        
        for batch in request.batches:
            if not batch:
                all_embeddings.append([])
                batch_times.append(0)
                continue
            
            batch_start = time.time()
            vectors = model.encode(batch, normalize_embeddings=True)
            batch_time = time.time() - batch_start
            
            all_embeddings.append(vectors.tolist())
            batch_times.append(round(batch_time, 3))
        
        _request_stats["embed_success"] += 1
        return BatchEmbedResponse(embeddings=all_embeddings, batch_times=batch_times)
    
    except Exception as e:
        _request_stats["embed_failed"] += 1
        raise HTTPException(status_code=500, detail=f"Batch embedding failed: {e}")


# ============================================
# é‡æ’åºç«¯ç‚¹
# ============================================

class RerankRequest(BaseModel):
    query: str
    documents: List[str]


class RerankResponse(BaseModel):
    scores: List[float]


@app.post("/rerank", response_model=RerankResponse)
async def rerank(request: RerankRequest) -> RerankResponse:
    """æ–‡æ¡£é‡æ’åº"""
    global _request_stats
    _request_stats["rerank_total"] += 1
    
    start_time = time.time()
    
    if not request.documents:
        return RerankResponse(scores=[])
    
    try:
        reranker = _get_reranker()
        pairs = [[request.query, doc or ""] for doc in request.documents]
        
        if hasattr(reranker, "compute_score"):
            scores = reranker.compute_score(pairs)
        else:
            scores = reranker.predict(pairs)
        
        try:
            scores_list = list(scores)
        except TypeError:
            scores_list = [scores]
        
        scores_list = [float(s) for s in scores_list]
        
        elapsed = time.time() - start_time
        logger.info(f"[Rerank] âœ… å®Œæˆ | æ–‡æ¡£æ•°: {len(request.documents)} | è€—æ—¶: {elapsed:.3f}s")
        
        _request_stats["rerank_success"] += 1
        return RerankResponse(scores=scores_list)
    
    except Exception as e:
        _request_stats["rerank_failed"] += 1
        logger.error(f"[Rerank] âŒ å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Rerank failed: {e}")


# ============================================
# å¯åŠ¨å…¥å£
# ============================================

if __name__ == "__main__":
    import uvicorn
    
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    
    logger.info(f"å¯åŠ¨ GPU Server | host={host} | port={port}")
    uvicorn.run(app, host=host, port=port)
