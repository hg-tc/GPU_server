import logging
import os
import tempfile
import time
from datetime import datetime
from typing import List
from pathlib import Path

from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer, CrossEncoder
import torch

from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

# é…ç½® Hugging Face ç¦»çº¿æ¨¡å¼ï¼ˆå¦‚æœæ¨¡å‹å·²å®Œå…¨ä¸‹è½½ï¼Œå¯ä»¥é¿å…ç½‘ç»œè¯·æ±‚ï¼‰
# æ³¨æ„ï¼šå¯ç”¨åå¦‚æœæ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´å¯èƒ½ä¼šå¤±è´¥
HF_HUB_OFFLINE = os.getenv("HF_HUB_OFFLINE", "0").lower() in {"1", "true", "yes"}
if HF_HUB_OFFLINE:
    # å®Œå…¨ç¦ç”¨ Hugging Face Hub çš„ç½‘ç»œè¯·æ±‚
    os.environ["HF_HUB_OFFLINE"] = "1"
    os.environ["TRANSFORMERS_OFFLINE"] = "1"
    os.environ["HF_DATASETS_OFFLINE"] = "1"
    # ç¦ç”¨é‡è¯•æœºåˆ¶
    os.environ["HF_HUB_DISABLE_EXPERIMENTAL_WARNING"] = "1"
    # ç¦ç”¨ç‰ˆæœ¬æ£€æŸ¥
    os.environ["HF_HUB_DISABLE_VERSION_CHECK"] = "1"
else:
    # é…ç½® Hugging Face é•œåƒæºï¼ˆå¦‚æœæœªè®¾ç½®ï¼‰
    if not os.getenv("HF_ENDPOINT"):
        # ä½¿ç”¨ hf-mirror.com ä½œä¸ºé»˜è®¤é•œåƒæº
        os.environ["HF_ENDPOINT"] = os.getenv("HF_MIRROR_ENDPOINT", "https://hf-mirror.com")

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
def setup_logging():
    """é…ç½®è¯¦ç»†çš„æ—¥å¿—ç³»ç»Ÿ"""
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # æ—¥å¿—æ ¼å¼
    log_format = "%(asctime)s [%(levelname)-8s] [%(name)s] %(message)s"
    date_format = "%Y-%m-%d %H:%M:%S"
    
    # æ–‡ä»¶å¤„ç†å™¨ - è¯¦ç»†æ—¥å¿—
    file_handler = logging.FileHandler(
        log_dir / "gpu_server.log",
        encoding="utf-8",
        mode="a"
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # æ–‡ä»¶å¤„ç†å™¨ - é”™è¯¯æ—¥å¿—
    error_handler = logging.FileHandler(
        log_dir / "gpu_server_error.log",
        encoding="utf-8",
        mode="a"
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.handlers.clear()
    root_logger.addHandler(file_handler)
    root_logger.addHandler(error_handler)
    root_logger.addHandler(console_handler)
    
    # é…ç½®åº”ç”¨æ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger("gpu_pdf_server")
    logger.setLevel(logging.DEBUG)
    
    # é™ä½ç¬¬ä¸‰æ–¹åº“çš„æ—¥å¿—çº§åˆ«ï¼ˆå‡å°‘æ—¥å¿—å™ªéŸ³ï¼‰
    logging.getLogger("uvicorn").setLevel(logging.INFO)
    logging.getLogger("uvicorn.access").setLevel(logging.INFO)
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("httpcore").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)  # å‡å°‘ Hugging Face ç½‘ç»œè¯·æ±‚çš„ DEBUG æ—¥å¿—
    logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)
    logging.getLogger("transformers").setLevel(logging.WARNING)  # å‡å°‘ transformers åº“çš„è¯¦ç»†æ—¥å¿—
    logging.getLogger("huggingface_hub").setLevel(logging.WARNING)  # å‡å°‘ Hugging Face Hub çš„è¯¦ç»†æ—¥å¿—
    
    # å¦‚æœå¯ç”¨ç¦»çº¿æ¨¡å¼ï¼Œå®Œå…¨ç¦ç”¨ç½‘ç»œç›¸å…³æ—¥å¿—
    if os.getenv("HF_HUB_OFFLINE") == "1":
        logging.getLogger("urllib3").setLevel(logging.ERROR)
        logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)
        logging.getLogger("huggingface_hub").setLevel(logging.ERROR)
    
    return logger

logger = setup_logging()

# è®°å½•ç¦»çº¿æ¨¡å¼çŠ¶æ€
if os.getenv("HF_HUB_OFFLINE", "0") == "1":
    logger.info("=" * 60)
    logger.info("ğŸ“´ Hugging Face ç¦»çº¿æ¨¡å¼å·²å¯ç”¨")
    logger.info("   æ¨¡å‹å°†ä»æœ¬åœ°ç¼“å­˜åŠ è½½ï¼Œä¸ä¼šè¿›è¡Œç½‘ç»œè¯·æ±‚")
    logger.info("=" * 60)

# å¯åŠ¨æ—¶æ£€æµ‹å¹¶è®°å½•GPUä¿¡æ¯
try:
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        logger.info("=" * 60)
        logger.info("ğŸš€ GPU æ£€æµ‹ä¿¡æ¯:")
        logger.info(f"  âœ… CUDAå¯ç”¨: æ˜¯")
        logger.info(f"  ğŸ“¦ CUDAç‰ˆæœ¬: {torch.version.cuda}")
        logger.info(f"  ğŸ”§ PyTorchç‰ˆæœ¬: {torch.__version__}")
        logger.info(f"  ğŸ® GPUæ•°é‡: {gpu_count}")
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            logger.info(f"  ğŸ¯ GPU {i}: {props.name}")
            logger.info(f"     æ˜¾å­˜: {props.total_memory / 1024**3:.2f} GB")
            logger.info(f"     è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        logger.info("=" * 60)
    else:
        logger.warning("=" * 60)
        logger.warning("âš ï¸  CUDAä¸å¯ç”¨ï¼Œæ¨¡å‹å°†ä½¿ç”¨CPUè¿è¡Œï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
        logger.warning("=" * 60)
except Exception as e:
    logger.warning(f"GPUæ£€æµ‹å¤±è´¥: {e}")

app = FastAPI(
    title="GPU Model Server",
    version="0.1.0",
    description="Offload marker-pdf PDF->Markdown, embeddings, and reranking to a dedicated server",
)

# è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
class RequestLoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        client_ip = request.client.host if request.client else "unknown"
        
        # è®°å½•è¯·æ±‚å¼€å§‹
        logger.info(f"[è¯·æ±‚å¼€å§‹] {request.method} {request.url.path} | å®¢æˆ·ç«¯: {client_ip}")
        
        try:
            response = await call_next(request)
            process_time = time.time() - start_time
            
            # è®°å½•è¯·æ±‚å®Œæˆ
            status_code = response.status_code
            status_emoji = "âœ…" if 200 <= status_code < 300 else "âš ï¸" if 300 <= status_code < 400 else "âŒ"
            logger.info(
                f"[è¯·æ±‚å®Œæˆ] {status_emoji} {request.method} {request.url.path} | "
                f"çŠ¶æ€ç : {status_code} | å¤„ç†æ—¶é—´: {process_time:.3f}s | å®¢æˆ·ç«¯: {client_ip}"
            )
            
            return response
        except Exception as e:
            process_time = time.time() - start_time
            logger.error(
                f"[è¯·æ±‚å¼‚å¸¸] âŒ {request.method} {request.url.path} | "
                f"é”™è¯¯: {str(e)} | å¤„ç†æ—¶é—´: {process_time:.3f}s | å®¢æˆ·ç«¯: {client_ip}",
                exc_info=True
            )
            raise

app.add_middleware(RequestLoggingMiddleware)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

_converter = None
_embedder: SentenceTransformer | None = None
_reranker = None


def _get_converter() -> PdfConverter:
    global _converter
    if _converter is None:
        device = _get_device()
        logger.info(f"Initializing marker-pdf models on GPU server... | device={device}")
        
        if device == "cuda":
            logger.info(f"ä½¿ç”¨GPUåŠ é€ŸPDFè½¬æ¢ | GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}")
        else:
            logger.warning("ä½¿ç”¨CPUè¿è¡ŒPDFè½¬æ¢ï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
        
        models = create_model_dict()
        use_llm_env = os.getenv("MARKER_USE_LLM", "false").lower()
        use_llm = use_llm_env in {"1", "true", "yes"}
        pdftext_workers = int(os.getenv("PDFTEXT_WORKERS", "1"))

        _converter = PdfConverter(
            config={
                "pdftext_workers": pdftext_workers,
                "use_llm": use_llm,
            },
            artifact_dict=models,
            processor_list=None,
            renderer=None,
            llm_service=None,
        )
        logger.info(f"marker-pdf initialized successfully on GPU server | device={device}")
    return _converter


def _get_device() -> str:
    """æ™ºèƒ½æ£€æµ‹å¹¶è¿”å›æœ€ä½³è®¾å¤‡ï¼ˆGPUä¼˜å…ˆï¼‰"""
    # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨CPU
    force_cpu = os.getenv("FORCE_CPU", "0").lower() in {"1", "true", "yes"}
    if force_cpu:
        return "cpu"
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    cuda_available = torch.cuda.is_available()
    
    # å¦‚æœè®¾ç½®äº†CUDA_VISIBLE_DEVICESä¸”ä¸ä¸ºç©ºï¼Œä½¿ç”¨CUDA
    cuda_visible = os.getenv("CUDA_VISIBLE_DEVICES")
    if cuda_visible is not None and cuda_visible.strip() != "":
        if cuda_available:
            return "cuda"
        else:
            logger.warning(f"CUDA_VISIBLE_DEVICESè®¾ç½®ä¸º'{cuda_visible}'ä½†CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            return "cpu"
    
    # å¦‚æœå¼ºåˆ¶ä½¿ç”¨CUDA
    force_cuda = os.getenv("FORCE_CUDA", "0").lower() in {"1", "true", "yes"}
    if force_cuda:
        if cuda_available:
            return "cuda"
        else:
            logger.warning("FORCE_CUDA=1ä½†CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            return "cpu"
    
    # è‡ªåŠ¨æ£€æµ‹ï¼šå¦‚æœCUDAå¯ç”¨ï¼Œä¼˜å…ˆä½¿ç”¨GPU
    if cuda_available:
        device_count = torch.cuda.device_count()
        if device_count > 0:
            device_name = torch.cuda.get_device_name(0)
            logger.info(f"è‡ªåŠ¨æ£€æµ‹åˆ°GPU: {device_name} (è®¾å¤‡æ•°é‡: {device_count})")
            return "cuda"
    
    return "cpu"


def _get_embedder() -> SentenceTransformer:
    global _embedder
    if _embedder is None:
        model_name = os.getenv("EMBED_MODEL_NAME", "BAAI/bge-large-zh-v1.5")
        device = _get_device()
        logger.info(f"Initializing embedding model on GPU server: {model_name}, device={device}")
        
        if device == "cuda":
            logger.info(f"ä½¿ç”¨GPUåŠ é€ŸåµŒå…¥æ¨¡å‹ | GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}")
        else:
            logger.warning("ä½¿ç”¨CPUè¿è¡ŒåµŒå…¥æ¨¡å‹ï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
        
        # åœ¨ç¦»çº¿æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨æœ¬åœ°ç¼“å­˜è·¯å¾„
        if os.getenv("HF_HUB_OFFLINE") == "1":
            # å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
            cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
            logger.info(f"ç¦»çº¿æ¨¡å¼ï¼šå°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹: {model_name}")
        
        _embedder = SentenceTransformer(model_name, device=device)
        
        # éªŒè¯å®é™…ä½¿ç”¨çš„è®¾å¤‡
        if hasattr(_embedder, '_modules') and len(_embedder._modules) > 0:
            first_module = list(_embedder._modules.values())[0]
            if hasattr(first_module, 'device'):
                actual_device = str(first_module.device)
                logger.info(f"åµŒå…¥æ¨¡å‹å®é™…è¿è¡Œè®¾å¤‡: {actual_device}")
        
        logger.info("Embedding model initialized successfully")
    return _embedder


def _get_reranker():
    global _reranker
    if _reranker is None:
        model_name = os.getenv("RERANKER_MODEL_NAME", "BAAI/bge-reranker-v2-m3")
        device = _get_device()
        logger.info(f"Initializing reranker model on GPU server: {model_name}, device={device}")
        
        if device == "cuda":
            logger.info(f"ä½¿ç”¨GPUåŠ é€Ÿé‡æ’åºæ¨¡å‹ | GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}")
        else:
            logger.warning("ä½¿ç”¨CPUè¿è¡Œé‡æ’åºæ¨¡å‹ï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
        
        try:
            from FlagEmbedding import FlagReranker

            use_fp16 = device == "cuda" and torch.cuda.is_available()
            _reranker = FlagReranker(model_name, use_fp16=use_fp16, device=device)
            logger.info(f"FlagReranker initialized successfully | device={device}, fp16={use_fp16}")
        except ImportError:
            logger.warning("FlagEmbedding not available, falling back to CrossEncoder")
            _reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-12-v2", device=device)
            logger.info(f"CrossEncoder initialized successfully | device={device}")
    return _reranker


class EmbedRequest(BaseModel):
    texts: List[str]


class EmbedResponse(BaseModel):
    embeddings: List[List[float]]


class RerankRequest(BaseModel):
    query: str
    documents: List[str]


class RerankResponse(BaseModel):
    scores: List[float]


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    logger.debug("[å¥åº·æ£€æŸ¥] æ”¶åˆ°å¥åº·æ£€æŸ¥è¯·æ±‚")
    return {"status": "ok"}


@app.post("/pdf_to_markdown")
async def pdf_to_markdown(file: UploadFile = File(...)):
    """Convert uploaded PDF to Markdown using marker-pdf on this server."""
    task_start_time = time.time()
    file_size = 0
    tmp_path = None
    
    try:
        logger.info(f"[PDFè½¬æ¢ä»»åŠ¡] å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename}")
        
        if not file.filename.lower().endswith(".pdf"):
            logger.warning(f"[PDFè½¬æ¢ä»»åŠ¡] æ–‡ä»¶æ ¼å¼é”™è¯¯: {file.filename}")
            raise HTTPException(status_code=400, detail="Only PDF files are supported")

        # Save upload to a temporary PDF file
        with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp:
            tmp_path = tmp.name
            content = await file.read()
            file_size = len(content)
            
            if not content:
                logger.error(f"[PDFè½¬æ¢ä»»åŠ¡] æ–‡ä»¶ä¸ºç©º: {file.filename}")
                raise HTTPException(status_code=400, detail="Empty PDF file")
            
            tmp.write(content)
            logger.info(f"[PDFè½¬æ¢ä»»åŠ¡] æ–‡ä»¶å·²ä¿å­˜ | æ–‡ä»¶å: {file.filename} | å¤§å°: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)")

        # åŠ è½½è½¬æ¢å™¨
        load_start = time.time()
        converter = _get_converter()
        load_time = time.time() - load_start
        if load_time > 0.1:
            logger.info(f"[PDFè½¬æ¢ä»»åŠ¡] è½¬æ¢å™¨åŠ è½½æ—¶é—´: {load_time:.3f}s")

        # æ‰§è¡Œè½¬æ¢
        convert_start = time.time()
        logger.info(f"[PDFè½¬æ¢ä»»åŠ¡] å¼€å§‹PDFè½¬æ¢: {file.filename}")
        rendered = converter(tmp_path)
        convert_time = time.time() - convert_start
        logger.info(f"[PDFè½¬æ¢ä»»åŠ¡] PDFè½¬æ¢å®Œæˆ | è½¬æ¢æ—¶é—´: {convert_time:.3f}s")

        # æå–æ–‡æœ¬
        extract_start = time.time()
        markdown, _, _ = text_from_rendered(rendered)
        extract_time = time.time() - extract_start
        markdown_size = len(markdown) if markdown else 0
        
        if not markdown:
            logger.error(f"[PDFè½¬æ¢ä»»åŠ¡] è½¬æ¢ç»“æœä¸ºç©º: {file.filename}")
            raise HTTPException(status_code=500, detail="Empty Markdown output from marker-pdf")

        total_time = time.time() - task_start_time
        logger.info(
            f"[PDFè½¬æ¢ä»»åŠ¡] âœ… ä»»åŠ¡å®Œæˆ | æ–‡ä»¶å: {file.filename} | "
            f"è¾“å…¥å¤§å°: {file_size:,} bytes | è¾“å‡ºå¤§å°: {markdown_size:,} chars | "
            f"æ€»è€—æ—¶: {total_time:.3f}s (è½¬æ¢: {convert_time:.3f}s, æå–: {extract_time:.3f}s)"
        )

        return {
            "content": markdown,
            "conversion_method": "marker-pdf",
            "file_name": file.filename,
        }

    except HTTPException:
        raise
    except Exception as e:
        total_time = time.time() - task_start_time
        logger.error(
            f"[PDFè½¬æ¢ä»»åŠ¡] âŒ è½¬æ¢å¤±è´¥ | æ–‡ä»¶å: {file.filename} | "
            f"æ–‡ä»¶å¤§å°: {file_size:,} bytes | è€—æ—¶: {total_time:.3f}s | é”™è¯¯: {str(e)}",
            exc_info=True
        )
        raise HTTPException(status_code=500, detail=f"Conversion failed: {e}")
    finally:
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
                logger.debug(f"[PDFè½¬æ¢ä»»åŠ¡] ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤: {tmp_path}")
            except Exception as e:
                logger.warning(f"[PDFè½¬æ¢ä»»åŠ¡] åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {tmp_path}, é”™è¯¯: {e}")


@app.post("/embed", response_model=EmbedResponse)
async def embed(request: EmbedRequest) -> EmbedResponse:
    """Embed a batch of texts using a shared SentenceTransformer model on this server."""
    task_start_time = time.time()
    
    if not request.texts:
        logger.warning("[åµŒå…¥ä»»åŠ¡] è¯·æ±‚æ–‡æœ¬åˆ—è¡¨ä¸ºç©º")
        return EmbedResponse(embeddings=[])

    try:
        text_count = len(request.texts)
        total_chars = sum(len(text) for text in request.texts)
        logger.info(
            f"[åµŒå…¥ä»»åŠ¡] å¼€å§‹å¤„ç† | æ–‡æœ¬æ•°é‡: {text_count} | "
            f"æ€»å­—ç¬¦æ•°: {total_chars:,} | å¹³å‡é•¿åº¦: {total_chars//text_count if text_count > 0 else 0:,} chars"
        )
        
        # åŠ è½½æ¨¡å‹
        load_start = time.time()
        model = _get_embedder()
        load_time = time.time() - load_start
        if load_time > 0.1:
            logger.info(f"[åµŒå…¥ä»»åŠ¡] æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.3f}s")
        
        # æ‰§è¡ŒåµŒå…¥
        encode_start = time.time()
        vectors = model.encode(request.texts, normalize_embeddings=True)
        encode_time = time.time() - encode_start
        embeddings = vectors.tolist()
        
        embedding_dim = len(embeddings[0]) if embeddings else 0
        total_time = time.time() - task_start_time
        
        logger.info(
            f"[åµŒå…¥ä»»åŠ¡] âœ… ä»»åŠ¡å®Œæˆ | æ–‡æœ¬æ•°é‡: {text_count} | "
            f"åµŒå…¥ç»´åº¦: {embedding_dim} | ç¼–ç æ—¶é—´: {encode_time:.3f}s | æ€»è€—æ—¶: {total_time:.3f}s | "
            f"é€Ÿåº¦: {text_count/encode_time:.1f} texts/s"
        )
        
        return EmbedResponse(embeddings=embeddings)
    except Exception as e:
        total_time = time.time() - task_start_time
        logger.error(
            f"[åµŒå…¥ä»»åŠ¡] âŒ åµŒå…¥å¤±è´¥ | æ–‡æœ¬æ•°é‡: {len(request.texts)} | "
            f"è€—æ—¶: {total_time:.3f}s | é”™è¯¯: {str(e)}",
            exc_info=True
        )
        raise HTTPException(status_code=500, detail=f"Embedding failed: {e}")


@app.post("/rerank", response_model=RerankResponse)
async def rerank(request: RerankRequest) -> RerankResponse:
    """Rerank documents for a query using a shared reranker model on this server."""
    task_start_time = time.time()
    
    if not request.documents:
        logger.warning("[é‡æ’åºä»»åŠ¡] æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")
        return RerankResponse(scores=[])

    try:
        doc_count = len(request.documents)
        query_len = len(request.query)
        total_doc_chars = sum(len(doc or "") for doc in request.documents)
        logger.info(
            f"[é‡æ’åºä»»åŠ¡] å¼€å§‹å¤„ç† | æŸ¥è¯¢é•¿åº¦: {query_len:,} chars | "
            f"æ–‡æ¡£æ•°é‡: {doc_count} | æ€»æ–‡æ¡£å­—ç¬¦æ•°: {total_doc_chars:,}"
        )
        
        # åŠ è½½æ¨¡å‹
        load_start = time.time()
        reranker = _get_reranker()
        load_time = time.time() - load_start
        if load_time > 0.1:
            logger.info(f"[é‡æ’åºä»»åŠ¡] æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.3f}s")
        
        # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
        pairs = [[request.query, doc or ""] for doc in request.documents]
        
        # æ‰§è¡Œé‡æ’åº
        rerank_start = time.time()
        # FlagReranker uses compute_score, CrossEncoder uses predict
        if hasattr(reranker, "compute_score"):
            scores = reranker.compute_score(pairs)
        else:
            scores = reranker.predict(pairs)
        rerank_time = time.time() - rerank_start

        # Ensure we return a plain list of floats
        try:
            scores_list = list(scores)
        except TypeError:
            scores_list = [scores]
        
        scores_list = [float(s) for s in scores_list]
        max_score = max(scores_list) if scores_list else 0
        min_score = min(scores_list) if scores_list else 0
        total_time = time.time() - task_start_time
        
        logger.info(
            f"[é‡æ’åºä»»åŠ¡] âœ… ä»»åŠ¡å®Œæˆ | æ–‡æ¡£æ•°é‡: {doc_count} | "
            f"é‡æ’åºæ—¶é—´: {rerank_time:.3f}s | æ€»è€—æ—¶: {total_time:.3f}s | "
            f"åˆ†æ•°èŒƒå›´: [{min_score:.4f}, {max_score:.4f}] | "
            f"é€Ÿåº¦: {doc_count/rerank_time:.1f} pairs/s"
        )

        return RerankResponse(scores=scores_list)
    except Exception as e:
        total_time = time.time() - task_start_time
        logger.error(
            f"[é‡æ’åºä»»åŠ¡] âŒ é‡æ’åºå¤±è´¥ | æ–‡æ¡£æ•°é‡: {len(request.documents)} | "
            f"è€—æ—¶: {total_time:.3f}s | é”™è¯¯: {str(e)}",
            exc_info=True
        )
        raise HTTPException(status_code=500, detail=f"Rerank failed: {e}")
