import logging
import os
import tempfile
import time
from datetime import datetime
from typing import List
from pathlib import Path

from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.exceptions import HTTPException as StarletteHTTPException
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer, CrossEncoder
import torch

from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

# é…ç½® Hugging Face ç¦»çº¿æ¨¡å¼ï¼ˆå¦‚æœæ¨¡å‹å·²å®Œå…¨ä¸‹è½½ï¼Œå¯ä»¥é¿å…ç½‘ç»œè¯·æ±‚ï¼‰
# æ³¨æ„ï¼šå¯ç”¨åå¦‚æœæ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´å¯èƒ½ä¼šå¤±è´¥
HF_HUB_OFFLINE = os.getenv("HF_HUB_OFFLINE", "0").lower() in {"1", "true", "yes"}
if HF_HUB_OFFLINE:
    # å®Œå…¨ç¦ç”¨ Hugging Face Hub çš„ç½‘ç»œè¯·æ±‚
    os.environ["HF_HUB_OFFLINE"] = "1"
    os.environ["TRANSFORMERS_OFFLINE"] = "1"
    os.environ["HF_DATASETS_OFFLINE"] = "1"
    # ç¦ç”¨é‡è¯•æœºåˆ¶
    os.environ["HF_HUB_DISABLE_EXPERIMENTAL_WARNING"] = "1"
    # ç¦ç”¨ç‰ˆæœ¬æ£€æŸ¥
    os.environ["HF_HUB_DISABLE_VERSION_CHECK"] = "1"
    # ç¦ç”¨é¥æµ‹å’Œç½‘ç»œè¯·æ±‚
    os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
    os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
    # ç¡®ä¿ transformers åº“å®Œå…¨ç¦»çº¿
    os.environ["TRANSFORMERS_OFFLINE"] = "1"
    
    # åœ¨ç¦»çº¿æ¨¡å¼ä¸‹ï¼Œmonkey patch huggingface_hub çš„ model_info å‡½æ•°
    # ä»¥é¿å… transformers åº“åœ¨æ£€æŸ¥æ¨¡å‹ä¿¡æ¯æ—¶è§¦å‘ç½‘ç»œè¯·æ±‚
    try:
        import huggingface_hub
        from huggingface_hub import hf_api
        
        # ä¿å­˜åŸå§‹å‡½æ•°
        _original_model_info = hf_api.HfApi.model_info
        
        def _offline_model_info(self, repo_id, *args, **kwargs):
            """ç¦»çº¿æ¨¡å¼ä¸‹çš„ model_infoï¼Œè¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ¨¡å‹ä¿¡æ¯å¯¹è±¡"""
            from huggingface_hub.hf_api import ModelInfo
            # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„ ModelInfo å¯¹è±¡ï¼Œé¿å…ç½‘ç»œè¯·æ±‚
            # è¿™å¯èƒ½ä¼šåœ¨æŸäº›æƒ…å†µä¸‹å¤±è´¥ï¼Œä½†è‡³å°‘ä¸ä¼šè§¦å‘ç½‘ç»œè¯·æ±‚
            raise RuntimeError(
                f"ç¦»çº¿æ¨¡å¼å·²å¯ç”¨ï¼Œæ— æ³•è·å–æ¨¡å‹ä¿¡æ¯: {repo_id}\n"
                f"è¯·ç¡®ä¿æ¨¡å‹å·²å®Œå…¨ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜ï¼Œæˆ–è®¾ç½® HF_HUB_OFFLINE=0 ä»¥å…è®¸ç½‘ç»œè®¿é—®ã€‚"
            )
        
        # æ›¿æ¢å‡½æ•°
        hf_api.HfApi.model_info = _offline_model_info
    except Exception:
        # å¦‚æœ monkey patch å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ç¯å¢ƒå˜é‡æ–¹å¼
        pass
else:
    # é…ç½® Hugging Face é•œåƒæºï¼ˆå¦‚æœæœªè®¾ç½®ï¼‰
    if not os.getenv("HF_ENDPOINT"):
        # ä½¿ç”¨ hf-mirror.com ä½œä¸ºé»˜è®¤é•œåƒæº
        os.environ["HF_ENDPOINT"] = os.getenv("HF_MIRROR_ENDPOINT", "https://hf-mirror.com")

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
def setup_logging():
    """é…ç½®è¯¦ç»†çš„æ—¥å¿—ç³»ç»Ÿ"""
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # æ—¥å¿—æ ¼å¼
    log_format = "%(asctime)s [%(levelname)-8s] [%(name)s] %(message)s"
    date_format = "%Y-%m-%d %H:%M:%S"
    
    # æ–‡ä»¶å¤„ç†å™¨ - è¯¦ç»†æ—¥å¿—
    file_handler = logging.FileHandler(
        log_dir / "gpu_server.log",
        encoding="utf-8",
        mode="a"
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # æ–‡ä»¶å¤„ç†å™¨ - é”™è¯¯æ—¥å¿—
    error_handler = logging.FileHandler(
        log_dir / "gpu_server_error.log",
        encoding="utf-8",
        mode="a"
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.handlers.clear()
    root_logger.addHandler(file_handler)
    root_logger.addHandler(error_handler)
    root_logger.addHandler(console_handler)
    
    # é…ç½®åº”ç”¨æ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger("gpu_pdf_server")
    logger.setLevel(logging.DEBUG)
    
    # é™ä½ç¬¬ä¸‰æ–¹åº“çš„æ—¥å¿—çº§åˆ«ï¼ˆå‡å°‘æ—¥å¿—å™ªéŸ³ï¼‰
    logging.getLogger("uvicorn").setLevel(logging.INFO)
    logging.getLogger("uvicorn.access").setLevel(logging.INFO)
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("httpcore").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)  # å‡å°‘ Hugging Face ç½‘ç»œè¯·æ±‚çš„ DEBUG æ—¥å¿—
    logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)
    logging.getLogger("transformers").setLevel(logging.WARNING)  # å‡å°‘ transformers åº“çš„è¯¦ç»†æ—¥å¿—
    logging.getLogger("huggingface_hub").setLevel(logging.WARNING)  # å‡å°‘ Hugging Face Hub çš„è¯¦ç»†æ—¥å¿—
    
    # å¦‚æœå¯ç”¨ç¦»çº¿æ¨¡å¼ï¼Œå®Œå…¨ç¦ç”¨ç½‘ç»œç›¸å…³æ—¥å¿—
    if os.getenv("HF_HUB_OFFLINE") == "1":
        logging.getLogger("urllib3").setLevel(logging.ERROR)
        logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)
        logging.getLogger("huggingface_hub").setLevel(logging.ERROR)
    
    return logger

logger = setup_logging()

# è®°å½•ç¦»çº¿æ¨¡å¼çŠ¶æ€
if os.getenv("HF_HUB_OFFLINE", "0") == "1":
    logger.info("=" * 60)
    logger.info("ğŸ“´ Hugging Face ç¦»çº¿æ¨¡å¼å·²å¯ç”¨")
    logger.info("   æ¨¡å‹å°†ä»æœ¬åœ°ç¼“å­˜åŠ è½½ï¼Œä¸ä¼šè¿›è¡Œç½‘ç»œè¯·æ±‚")
    logger.info("=" * 60)

# å¯åŠ¨æ—¶æ£€æµ‹å¹¶è®°å½•GPUä¿¡æ¯
try:
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        logger.info("=" * 60)
        logger.info("ğŸš€ GPU æ£€æµ‹ä¿¡æ¯:")
        logger.info(f"  âœ… CUDAå¯ç”¨: æ˜¯")
        logger.info(f"  ğŸ“¦ CUDAç‰ˆæœ¬: {torch.version.cuda}")
        logger.info(f"  ğŸ”§ PyTorchç‰ˆæœ¬: {torch.__version__}")
        logger.info(f"  ğŸ® GPUæ•°é‡: {gpu_count}")
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            logger.info(f"  ğŸ¯ GPU {i}: {props.name}")
            logger.info(f"     æ˜¾å­˜: {props.total_memory / 1024**3:.2f} GB")
            logger.info(f"     è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        logger.info("=" * 60)
    else:
        logger.warning("=" * 60)
        logger.warning("âš ï¸  CUDAä¸å¯ç”¨ï¼Œæ¨¡å‹å°†ä½¿ç”¨CPUè¿è¡Œï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
        logger.warning("=" * 60)
except Exception as e:
    logger.warning(f"GPUæ£€æµ‹å¤±è´¥: {e}")

app = FastAPI(
    title="GPU Model Server",
    version="0.1.0",
    description="Offload marker-pdf PDF->Markdown, embeddings, and reranking to a dedicated server",
)

# è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
class RequestLoggingMiddleware(BaseHTTPMiddleware):
    # è¶…æ—¶é˜ˆå€¼ï¼ˆç§’ï¼‰
    WARNING_TIMEOUT = 60  # è¶…è¿‡60ç§’å‘å‡ºè­¦å‘Š
    CRITICAL_TIMEOUT = 300  # è¶…è¿‡300ç§’å‘å‡ºä¸¥é‡è­¦å‘Š
    
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        client_ip = request.client.host if request.client else "unknown"
        
        # è®°å½•è¯·æ±‚å¼€å§‹
        logger.info(f"[è¯·æ±‚å¼€å§‹] {request.method} {request.url.path} | å®¢æˆ·ç«¯: {client_ip}")
        
        try:
            response = await call_next(request)
            process_time = time.time() - start_time
            
            # è¶…æ—¶æ£€æµ‹å’Œè­¦å‘Š
            if process_time > self.CRITICAL_TIMEOUT:
                logger.warning(
                    f"[è¯·æ±‚è¶…æ—¶è­¦å‘Š] âš ï¸âš ï¸âš ï¸ {request.method} {request.url.path} | "
                    f"å¤„ç†æ—¶é—´è¿‡é•¿: {process_time:.3f}s (è¶…è¿‡{self.CRITICAL_TIMEOUT}s) | å®¢æˆ·ç«¯: {client_ip}"
                )
            elif process_time > self.WARNING_TIMEOUT:
                logger.warning(
                    f"[è¯·æ±‚è¶…æ—¶è­¦å‘Š] âš ï¸ {request.method} {request.url.path} | "
                    f"å¤„ç†æ—¶é—´è¾ƒé•¿: {process_time:.3f}s (è¶…è¿‡{self.WARNING_TIMEOUT}s) | å®¢æˆ·ç«¯: {client_ip}"
                )
            
            # è®°å½•è¯·æ±‚å®Œæˆ
            status_code = response.status_code
            status_emoji = "âœ…" if 200 <= status_code < 300 else "âš ï¸" if 300 <= status_code < 400 else "âŒ"
            logger.info(
                f"[è¯·æ±‚å®Œæˆ] {status_emoji} {request.method} {request.url.path} | "
                f"çŠ¶æ€ç : {status_code} | å¤„ç†æ—¶é—´: {process_time:.3f}s | å®¢æˆ·ç«¯: {client_ip}"
            )
            
            return response
        except Exception as e:
            process_time = time.time() - start_time
            logger.error(
                f"[è¯·æ±‚å¼‚å¸¸] âŒ {request.method} {request.url.path} | "
                f"é”™è¯¯: {str(e)} | å¤„ç†æ—¶é—´: {process_time:.3f}s | å®¢æˆ·ç«¯: {client_ip}",
                exc_info=True
            )
            # ç¡®ä¿å¼‚å¸¸è¢«æ­£ç¡®ä¼ æ’­ï¼Œè®©å…¨å±€å¼‚å¸¸å¤„ç†å™¨å¤„ç†
            raise

app.add_middleware(RequestLoggingMiddleware)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å¼‚å¸¸å¤„ç†å™¨ - ç¡®ä¿æ‰€æœ‰å¼‚å¸¸éƒ½èƒ½è¿”å›HTTPå“åº”
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """å…¨å±€å¼‚å¸¸å¤„ç†å™¨ï¼Œç¡®ä¿æ‰€æœ‰å¼‚å¸¸éƒ½èƒ½è¿”å›HTTPå“åº”è€Œä¸æ˜¯æ–­å¼€è¿æ¥"""
    logger.error(
        f"[å…¨å±€å¼‚å¸¸] âŒ {request.method} {request.url.path} | "
        f"æœªæ•è·çš„å¼‚å¸¸: {type(exc).__name__}: {str(exc)}",
        exc_info=True
    )
    return JSONResponse(
        status_code=500,
        content={
            "detail": f"Internal server error: {str(exc)}",
            "error_type": type(exc).__name__
        }
    )

@app.exception_handler(StarletteHTTPException)
async def http_exception_handler(request: Request, exc: StarletteHTTPException):
    """HTTPå¼‚å¸¸å¤„ç†å™¨"""
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail}
    )

_converter = None
_embedder: SentenceTransformer | None = None
_reranker = None
_ocr_engine = None


def _get_converter() -> PdfConverter:
    global _converter
    if _converter is None:
        device = _get_device()
        logger.info(f"Initializing marker-pdf models on GPU server... | device={device}")
        
        if device == "cuda":
            logger.info(f"ä½¿ç”¨GPUåŠ é€ŸPDFè½¬æ¢ | GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}")
        else:
            logger.warning("ä½¿ç”¨CPUè¿è¡ŒPDFè½¬æ¢ï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
        
        models = create_model_dict()
        use_llm_env = os.getenv("MARKER_USE_LLM", "false").lower()
        use_llm = use_llm_env in {"1", "true", "yes"}
        pdftext_workers = int(os.getenv("PDFTEXT_WORKERS", "1"))

        _converter = PdfConverter(
            config={
                "pdftext_workers": pdftext_workers,
                "use_llm": use_llm,
            },
            artifact_dict=models,
            processor_list=None,
            renderer=None,
            llm_service=None,
        )
        logger.info(f"marker-pdf initialized successfully on GPU server | device={device}")
    return _converter


def _get_device() -> str:
    """æ™ºèƒ½æ£€æµ‹å¹¶è¿”å›æœ€ä½³è®¾å¤‡ï¼ˆGPUä¼˜å…ˆï¼‰"""
    # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨CPU
    force_cpu = os.getenv("FORCE_CPU", "0").lower() in {"1", "true", "yes"}
    if force_cpu:
        return "cpu"
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    cuda_available = torch.cuda.is_available()
    
    # å¦‚æœè®¾ç½®äº†CUDA_VISIBLE_DEVICESä¸”ä¸ä¸ºç©ºï¼Œä½¿ç”¨CUDA
    cuda_visible = os.getenv("CUDA_VISIBLE_DEVICES")
    if cuda_visible is not None and cuda_visible.strip() != "":
        if cuda_available:
            return "cuda"
        else:
            logger.warning(f"CUDA_VISIBLE_DEVICESè®¾ç½®ä¸º'{cuda_visible}'ä½†CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            return "cpu"
    
    # å¦‚æœå¼ºåˆ¶ä½¿ç”¨CUDA
    force_cuda = os.getenv("FORCE_CUDA", "0").lower() in {"1", "true", "yes"}
    if force_cuda:
        if cuda_available:
            return "cuda"
        else:
            logger.warning("FORCE_CUDA=1ä½†CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            return "cpu"
    
    # è‡ªåŠ¨æ£€æµ‹ï¼šå¦‚æœCUDAå¯ç”¨ï¼Œä¼˜å…ˆä½¿ç”¨GPU
    if cuda_available:
        device_count = torch.cuda.device_count()
        if device_count > 0:
            device_name = torch.cuda.get_device_name(0)
            logger.info(f"è‡ªåŠ¨æ£€æµ‹åˆ°GPU: {device_name} (è®¾å¤‡æ•°é‡: {device_count})")
            return "cuda"
    
    return "cpu"


def _get_embedder() -> SentenceTransformer:
    global _embedder
    if _embedder is None:
        model_name = os.getenv("EMBED_MODEL_NAME", "BAAI/bge-large-zh-v1.5")
        device = _get_device()
        logger.info(f"Initializing embedding model on GPU server: {model_name}, device={device}")
        
        if device == "cuda":
            logger.info(f"ä½¿ç”¨GPUåŠ é€ŸåµŒå…¥æ¨¡å‹ | GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}")
        else:
            logger.warning("ä½¿ç”¨CPUè¿è¡ŒåµŒå…¥æ¨¡å‹ï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
        
        # åœ¨ç¦»çº¿æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨æœ¬åœ°ç¼“å­˜è·¯å¾„
        if os.getenv("HF_HUB_OFFLINE") == "1":
            # å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
            cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
            logger.info(f"ç¦»çº¿æ¨¡å¼ï¼šå°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹: {model_name}")
        
        _embedder = SentenceTransformer(model_name, device=device)
        
        # éªŒè¯å®é™…ä½¿ç”¨çš„è®¾å¤‡
        if hasattr(_embedder, '_modules') and len(_embedder._modules) > 0:
            first_module = list(_embedder._modules.values())[0]
            if hasattr(first_module, 'device'):
                actual_device = str(first_module.device)
                logger.info(f"åµŒå…¥æ¨¡å‹å®é™…è¿è¡Œè®¾å¤‡: {actual_device}")
        
        logger.info("Embedding model initialized successfully")
    return _embedder


def _get_reranker():
    global _reranker
    if _reranker is None:
        model_name = os.getenv("RERANKER_MODEL_NAME", "BAAI/bge-reranker-v2-m3")
        device = _get_device()
        logger.info(f"Initializing reranker model on GPU server: {model_name}, device={device}")
        
        if device == "cuda":
            logger.info(f"ä½¿ç”¨GPUåŠ é€Ÿé‡æ’åºæ¨¡å‹ | GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}")
        else:
            logger.warning("ä½¿ç”¨CPUè¿è¡Œé‡æ’åºæ¨¡å‹ï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
        
        try:
            from FlagEmbedding import FlagReranker

            use_fp16 = device == "cuda" and torch.cuda.is_available()
            
            # åœ¨ç¦»çº¿æ¨¡å¼ä¸‹ï¼Œå°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
            if os.getenv("HF_HUB_OFFLINE") == "1":
                # å°è¯•è·å–æœ¬åœ°ç¼“å­˜è·¯å¾„
                cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
                model_cache = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
                
                if os.path.exists(model_cache):
                    # æŸ¥æ‰¾æœ€æ–°çš„å¿«ç…§
                    snapshots_dir = os.path.join(model_cache, "snapshots")
                    if os.path.exists(snapshots_dir):
                        snapshots = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                        if snapshots:
                            # ä½¿ç”¨æœ€æ–°çš„å¿«ç…§
                            local_path = os.path.join(snapshots_dir, snapshots[-1])
                            logger.info(f"ç¦»çº¿æ¨¡å¼ï¼šä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹: {local_path}")
                            try:
                                _reranker = FlagReranker(local_path, use_fp16=use_fp16, device=device)
                                logger.info(f"FlagReranker initialized successfully from local cache | device={device}, fp16={use_fp16}")
                                return _reranker
                            except Exception as e:
                                error_msg = (
                                    f"ç¦»çº¿æ¨¡å¼ä¸‹ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹å¤±è´¥: {e}\n"
                                    f"æœ¬åœ°è·¯å¾„: {local_path}\n"
                                    f"è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ï¼Œæˆ–è®¾ç½® HF_HUB_OFFLINE=0 ä»¥å…è®¸ç½‘ç»œè®¿é—®ã€‚"
                                )
                                logger.error(error_msg)
                                raise RuntimeError(error_msg) from e
                
                # ç¦»çº¿æ¨¡å¼ä¸‹æœªæ‰¾åˆ°æœ¬åœ°ç¼“å­˜
                error_msg = (
                    f"ç¦»çº¿æ¨¡å¼ä¸‹æœªæ‰¾åˆ°æ¨¡å‹ {model_name} çš„æœ¬åœ°ç¼“å­˜ã€‚\n"
                    f"ç¼“å­˜ç›®å½•: {cache_dir}\n"
                    f"è¯·å…ˆä¸‹è½½æ¨¡å‹ï¼Œæˆ–è®¾ç½® HF_HUB_OFFLINE=0 ä»¥å…è®¸ç½‘ç»œè®¿é—®ã€‚\n"
                    f"ä¸‹è½½å‘½ä»¤: python -c \"from huggingface_hub import snapshot_download; snapshot_download('{model_name}')\""
                )
                logger.error(error_msg)
                raise RuntimeError(error_msg)
            
            # åœ¨çº¿æ¨¡å¼ï¼šæ­£å¸¸åŠ è½½
            _reranker = FlagReranker(model_name, use_fp16=use_fp16, device=device)
            logger.info(f"FlagReranker initialized successfully | device={device}, fp16={use_fp16}")
        except ImportError:
            logger.warning("FlagEmbedding not available, falling back to CrossEncoder")
            _reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-12-v2", device=device)
            logger.info(f"CrossEncoder initialized successfully | device={device}")
        except Exception as e:
            if os.getenv("HF_HUB_OFFLINE") == "1":
                error_msg = (
                    f"ç¦»çº¿æ¨¡å¼ä¸‹åŠ è½½é‡æ’åºæ¨¡å‹å¤±è´¥: {e}\n"
                    f"è¯·ç¡®ä¿æ¨¡å‹ {model_name} å·²å®Œå…¨ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜ã€‚\n"
                    f"å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ¨¡å‹ï¼š\n"
                    f"  python -c \"from huggingface_hub import snapshot_download; snapshot_download('{model_name}')\"\n"
                    f"æˆ–è€…è®¾ç½® HF_HUB_OFFLINE=0 ä»¥å…è®¸ç½‘ç»œè®¿é—®ã€‚"
                )
                logger.error(error_msg)
                raise RuntimeError(error_msg) from e
            else:
                raise
    return _reranker


def _get_ocr_engine():
    global _ocr_engine
    if _ocr_engine is None:
        try:
            from paddleocr import PaddleOCR
            import paddle
        except ImportError as e:
            logger.error("PaddleOCR not available on GPU server: %s", e)
            raise

        # æ£€æŸ¥ GPU é…ç½®
        use_gpu_env = os.getenv("GPU_OCR_USE_GPU", "1").lower()
        use_gpu = use_gpu_env in {"1", "true", "yes"}
        
        # æ–°ç‰ˆæœ¬çš„ PaddleOCR ä¸å†æ”¯æŒ use_gpu å‚æ•°
        # GPU ä½¿ç”¨ç”± PaddlePaddle è‡ªåŠ¨æ£€æµ‹ï¼Œæˆ–é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶
        if use_gpu:
            # è®¾ç½® PaddlePaddle ä½¿ç”¨ GPU
            try:
                if paddle.device.is_compiled_with_cuda():
                    # è®¾ç½®é»˜è®¤è®¾å¤‡ä¸º GPU
                    paddle.set_device('gpu')
                    logger.info("OCR engine will use GPU (PaddlePaddle CUDA enabled)")
                else:
                    logger.warning("PaddlePaddle æœªç¼–è¯‘ CUDA æ”¯æŒï¼ŒOCR å°†ä½¿ç”¨ CPU")
                    use_gpu = False
            except Exception as e:
                logger.warning(f"è®¾ç½® GPU è®¾å¤‡å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ CPU")
                use_gpu = False
        else:
            paddle.set_device('cpu')
            logger.info("OCR engine will use CPU (GPU_OCR_USE_GPU=0)")

        # åˆå§‹åŒ– PaddleOCRï¼ˆæ–°ç‰ˆæœ¬ APIï¼‰
        try:
            _ocr_engine = PaddleOCR(
                use_angle_cls=True,
                lang="ch",
            )
            logger.info("OCR engine initialized with PaddleOCR (use_gpu=%s)", use_gpu)
        except Exception as e:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•æ›´ç®€å•çš„åˆå§‹åŒ–
            logger.warning(f"ä½¿ç”¨æ ‡å‡†å‚æ•°åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°è¯•ç®€åŒ–åˆå§‹åŒ–")
            _ocr_engine = PaddleOCR(lang="ch")
            logger.info("OCR engine initialized with PaddleOCR (simplified, use_gpu=%s)", use_gpu)
    return _ocr_engine


class EmbedRequest(BaseModel):
    texts: List[str]


class EmbedResponse(BaseModel):
    embeddings: List[List[float]]


class RerankRequest(BaseModel):
    query: str
    documents: List[str]


class RerankResponse(BaseModel):
    scores: List[float]


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    logger.debug("[å¥åº·æ£€æŸ¥] æ”¶åˆ°å¥åº·æ£€æŸ¥è¯·æ±‚")
    return {"status": "ok"}


@app.post("/pdf_to_markdown")
async def pdf_to_markdown(file: UploadFile = File(...)):
    """Convert uploaded PDF to Markdown using marker-pdf on this server."""
    task_start_time = time.time()
    file_size = 0
    tmp_path = None
    
    try:
        logger.info(f"[PDFè½¬æ¢ä»»åŠ¡] å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename}")
        
        if not file.filename.lower().endswith(".pdf"):
            logger.warning(f"[PDFè½¬æ¢ä»»åŠ¡] æ–‡ä»¶æ ¼å¼é”™è¯¯: {file.filename}")
            raise HTTPException(status_code=400, detail="Only PDF files are supported")

        # Save upload to a temporary PDF file
        with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp:
            tmp_path = tmp.name
            content = await file.read()
            file_size = len(content)
            
            if not content:
                logger.error(f"[PDFè½¬æ¢ä»»åŠ¡] æ–‡ä»¶ä¸ºç©º: {file.filename}")
                raise HTTPException(status_code=400, detail="Empty PDF file")
            
            tmp.write(content)
            logger.info(f"[PDFè½¬æ¢ä»»åŠ¡] æ–‡ä»¶å·²ä¿å­˜ | æ–‡ä»¶å: {file.filename} | å¤§å°: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)")

        # åŠ è½½è½¬æ¢å™¨
        load_start = time.time()
        converter = _get_converter()
        load_time = time.time() - load_start
        if load_time > 0.1:
            logger.info(f"[PDFè½¬æ¢ä»»åŠ¡] è½¬æ¢å™¨åŠ è½½æ—¶é—´: {load_time:.3f}s")

        # æ‰§è¡Œè½¬æ¢
        convert_start = time.time()
        logger.info(f"[PDFè½¬æ¢ä»»åŠ¡] å¼€å§‹PDFè½¬æ¢: {file.filename}")
        rendered = converter(tmp_path)
        convert_time = time.time() - convert_start
        logger.info(f"[PDFè½¬æ¢ä»»åŠ¡] PDFè½¬æ¢å®Œæˆ | è½¬æ¢æ—¶é—´: {convert_time:.3f}s")

        # æå–æ–‡æœ¬
        extract_start = time.time()
        markdown, _, _ = text_from_rendered(rendered)
        extract_time = time.time() - extract_start
        markdown_size = len(markdown) if markdown else 0
        
        if not markdown:
            logger.error(f"[PDFè½¬æ¢ä»»åŠ¡] è½¬æ¢ç»“æœä¸ºç©º: {file.filename}")
            raise HTTPException(status_code=500, detail="Empty Markdown output from marker-pdf")

        total_time = time.time() - task_start_time
        logger.info(
            f"[PDFè½¬æ¢ä»»åŠ¡] âœ… ä»»åŠ¡å®Œæˆ | æ–‡ä»¶å: {file.filename} | "
            f"è¾“å…¥å¤§å°: {file_size:,} bytes | è¾“å‡ºå¤§å°: {markdown_size:,} chars | "
            f"æ€»è€—æ—¶: {total_time:.3f}s (è½¬æ¢: {convert_time:.3f}s, æå–: {extract_time:.3f}s)"
        )

        return {
            "content": markdown,
            "conversion_method": "marker-pdf",
            "file_name": file.filename,
        }

    except HTTPException:
        raise
    except Exception as e:
        total_time = time.time() - task_start_time
        logger.error(
            f"[PDFè½¬æ¢ä»»åŠ¡] âŒ è½¬æ¢å¤±è´¥ | æ–‡ä»¶å: {file.filename} | "
            f"æ–‡ä»¶å¤§å°: {file_size:,} bytes | è€—æ—¶: {total_time:.3f}s | é”™è¯¯: {str(e)}",
            exc_info=True
        )
        raise HTTPException(status_code=500, detail=f"Conversion failed: {e}")
    finally:
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
                logger.debug(f"[PDFè½¬æ¢ä»»åŠ¡] ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤: {tmp_path}")
            except Exception as e:
                logger.warning(f"[PDFè½¬æ¢ä»»åŠ¡] åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {tmp_path}, é”™è¯¯: {e}")


@app.post("/embed", response_model=EmbedResponse)
async def embed(request: EmbedRequest) -> EmbedResponse:
    """Embed a batch of texts using a shared SentenceTransformer model on this server."""
    task_start_time = time.time()
    
    if not request.texts:
        logger.warning("[åµŒå…¥ä»»åŠ¡] è¯·æ±‚æ–‡æœ¬åˆ—è¡¨ä¸ºç©º")
        return EmbedResponse(embeddings=[])
    
    try:
        text_count = len(request.texts)
        total_chars = sum(len(text) for text in request.texts)
        logger.info(
            f"[åµŒå…¥ä»»åŠ¡] å¼€å§‹å¤„ç† | æ–‡æœ¬æ•°é‡: {text_count} | "
            f"æ€»å­—ç¬¦æ•°: {total_chars:,} | å¹³å‡é•¿åº¦: {total_chars//text_count if text_count > 0 else 0:,} chars"
        )
        
        # åŠ è½½æ¨¡å‹
        load_start = time.time()
        try:
            model = _get_embedder()
        except Exception as model_error:
            logger.error(f"[åµŒå…¥ä»»åŠ¡] æ¨¡å‹åŠ è½½å¤±è´¥: {str(model_error)}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"Failed to load embedding model: {model_error}")
        
        load_time = time.time() - load_start
        if load_time > 0.1:
            logger.info(f"[åµŒå…¥ä»»åŠ¡] æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.3f}s")
        
        # æ‰§è¡ŒåµŒå…¥
        encode_start = time.time()
        try:
            vectors = model.encode(request.texts, normalize_embeddings=True)
        except Exception as encode_error:
            logger.error(f"[åµŒå…¥ä»»åŠ¡] ç¼–ç è¿‡ç¨‹å¤±è´¥: {str(encode_error)}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"Encoding failed: {encode_error}")
        
        encode_time = time.time() - encode_start
        
        # å®‰å…¨è½¬æ¢ä¸ºåˆ—è¡¨
        try:
            embeddings = vectors.tolist()
        except Exception as convert_error:
            logger.error(f"[åµŒå…¥ä»»åŠ¡] å‘é‡è½¬æ¢å¤±è´¥: {str(convert_error)}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"Vector conversion failed: {convert_error}")
        
        embedding_dim = len(embeddings[0]) if embeddings else 0
        total_time = time.time() - task_start_time
        
        logger.info(
            f"[åµŒå…¥ä»»åŠ¡] âœ… ä»»åŠ¡å®Œæˆ | æ–‡æœ¬æ•°é‡: {text_count} | "
            f"åµŒå…¥ç»´åº¦: {embedding_dim} | ç¼–ç æ—¶é—´: {encode_time:.3f}s | æ€»è€—æ—¶: {total_time:.3f}s | "
            f"é€Ÿåº¦: {text_count/encode_time:.1f} texts/s"
        )
        
        return EmbedResponse(embeddings=embeddings)
    except HTTPException:
        # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸ï¼Œç¡®ä¿å“åº”è¢«å‘é€
        raise
    except Exception as e:
        total_time = time.time() - task_start_time
        logger.error(
            f"[åµŒå…¥ä»»åŠ¡] âŒ åµŒå…¥å¤±è´¥ | æ–‡æœ¬æ•°é‡: {len(request.texts) if request.texts else 0} | "
            f"è€—æ—¶: {total_time:.3f}s | é”™è¯¯: {str(e)}",
            exc_info=True
        )
        # ç¡®ä¿è¿”å›HTTPå“åº”è€Œä¸æ˜¯è®©è¿æ¥æ–­å¼€
        raise HTTPException(status_code=500, detail=f"Embedding failed: {str(e)}")


@app.post("/rerank", response_model=RerankResponse)
async def rerank(request: RerankRequest) -> RerankResponse:
    """Rerank documents for a query using a shared reranker model on this server."""
    task_start_time = time.time()
    
    if not request.documents:
        logger.warning("[é‡æ’åºä»»åŠ¡] æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")
        return RerankResponse(scores=[])

    try:
        doc_count = len(request.documents)
        query_len = len(request.query)
        total_doc_chars = sum(len(doc or "") for doc in request.documents)
        logger.info(
            f"[é‡æ’åºä»»åŠ¡] å¼€å§‹å¤„ç† | æŸ¥è¯¢é•¿åº¦: {query_len:,} chars | "
            f"æ–‡æ¡£æ•°é‡: {doc_count} | æ€»æ–‡æ¡£å­—ç¬¦æ•°: {total_doc_chars:,}"
        )
        
        # åŠ è½½æ¨¡å‹
        load_start = time.time()
        reranker = _get_reranker()
        load_time = time.time() - load_start
        if load_time > 0.1:
            logger.info(f"[é‡æ’åºä»»åŠ¡] æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.3f}s")
        
        # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
        pairs = [[request.query, doc or ""] for doc in request.documents]
        
        # æ‰§è¡Œé‡æ’åº
        rerank_start = time.time()
        # FlagReranker uses compute_score, CrossEncoder uses predict
        if hasattr(reranker, "compute_score"):
            scores = reranker.compute_score(pairs)
        else:
            scores = reranker.predict(pairs)
        rerank_time = time.time() - rerank_start

        # Ensure we return a plain list of floats
        try:
            scores_list = list(scores)
        except TypeError:
            scores_list = [scores]
        
        scores_list = [float(s) for s in scores_list]
        max_score = max(scores_list) if scores_list else 0
        min_score = min(scores_list) if scores_list else 0
        total_time = time.time() - task_start_time
        
        logger.info(
            f"[é‡æ’åºä»»åŠ¡] âœ… ä»»åŠ¡å®Œæˆ | æ–‡æ¡£æ•°é‡: {doc_count} | "
            f"é‡æ’åºæ—¶é—´: {rerank_time:.3f}s | æ€»è€—æ—¶: {total_time:.3f}s | "
            f"åˆ†æ•°èŒƒå›´: [{min_score:.4f}, {max_score:.4f}] | "
            f"é€Ÿåº¦: {doc_count/rerank_time:.1f} pairs/s"
        )

        return RerankResponse(scores=scores_list)
    except Exception as e:
        total_time = time.time() - task_start_time
        logger.error(
            f"[é‡æ’åºä»»åŠ¡] âŒ é‡æ’åºå¤±è´¥ | æ–‡æ¡£æ•°é‡: {len(request.documents)} | "
            f"è€—æ—¶: {total_time:.3f}s | é”™è¯¯: {str(e)}",
            exc_info=True
        )
        raise HTTPException(status_code=500, detail=f"Rerank failed: {e}")


class OCRResponse(BaseModel):
    text: str
    confidence: float
    lines: List[str]
    confidences: List[float]
    boxes: List[List[List[float]]]


@app.post("/ocr_image", response_model=OCRResponse)
async def ocr_image(file: UploadFile = File(...)) -> OCRResponse:
    task_start_time = time.time()
    tmp_path = None

    try:
        if not file.filename:
            raise HTTPException(status_code=400, detail="Empty filename")

        suffix = os.path.splitext(file.filename)[1].lower() or ".png"

        with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
            tmp_path = tmp.name
            content = await file.read()
            if not content:
                raise HTTPException(status_code=400, detail="Empty image file")
            tmp.write(content)

        ocr_engine = _get_ocr_engine()

        ocr_result = ocr_engine.ocr(tmp_path, cls=True)

        lines: List[str] = []
        confidences: List[float] = []
        boxes: List[List[List[float]]] = []

        if ocr_result and ocr_result[0]:
            for item in ocr_result[0]:
                if not item or len(item) < 2:
                    continue
                box = item[0]
                text_info = item[1]
                if not text_info or len(text_info) < 2:
                    continue
                text = text_info[0]
                score = float(text_info[1])
                if text:
                    lines.append(text)
                    confidences.append(score)
                    boxes.append(box)

        full_text = "\n".join(lines)
        avg_conf = sum(confidences) / len(confidences) if confidences else 0.0
        elapsed = time.time() - task_start_time

        logger.info(
            "[OCRä»»åŠ¡] å®Œæˆ | æ–‡ä»¶å: %s | æ–‡æœ¬é•¿åº¦: %d | ç½®ä¿¡åº¦: %.4f | è€—æ—¶: %.3fs",
            file.filename,
            len(full_text),
            avg_conf,
            elapsed,
        )

        return OCRResponse(
            text=full_text,
            confidence=avg_conf,
            lines=lines,
            confidences=confidences,
            boxes=boxes,
        )
    except HTTPException:
        raise
    except Exception as e:
        elapsed = time.time() - task_start_time
        logger.error(
            "[OCRä»»åŠ¡] å¤±è´¥ | æ–‡ä»¶å: %s | è€—æ—¶: %.3fs | é”™è¯¯: %s",
            file.filename,
            elapsed,
            str(e),
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=f"OCR failed: {e}")
    finally:
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except Exception:
                pass
